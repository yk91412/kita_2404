

Stratify는 데이터를 분할할 때 각 클래스의 비율을 유지해


입력(features) : 학습할 때 사용하는 변수

  꽃잎의 길이와 너비

  집의 크기 위치 방의 개수

타겟(labels) : 모델이 예측하려는 결과 값

  꽃의 종류

  집의 가격

=========================================================

## 분류모델




train_tests_split random 11




[DecisionTreeClassifier] - 분류
[DecisionTreeRegresssir] - 회귀


** 결정 트리

(random forest의 기본 구성요소)



  <규칙>

  최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록
  
  균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요

  => 성능 보장을 위해 성능 튜닝을 통해 트리의 크기를 사전에 제한하는 것이 요구

  <균일도 측정하는 방법>

  엔트로피(주어진 데이터 집합의 혼잡도) 이용

    -> 다른 값이 섞여있으면 혼잡도 높음

  1. 정보 이득 지수

    1 - 엔트로피(혼잡도)지수

    정보 이득이 높은 속성기준으로 분할

  2. 지니계수

  불평등 지수를 나타내는 계수(0 평등, 1 불평등)
  -> 불확실성(즉, 얼마나 섞여있는지)을 보여줌

  0이라는건 불확실성이 0인 특성을 가진 객체끼리 잘 모여있음을 의미미

  <구조>

  규칙 노드(규칙 조건)

  리프 노드(결정된 클래스 값)

  서브 트리(새로운 규칙 조건마다 규칙 노드 기반의 서브트리 생성)


  <하이퍼 파라미터>

  학습이 시작되기 전 설정되는 매개변수로 구조와 성능에 영향을 줌


  * 최대 깊이(max_depth)

  : 트리의 최대 깊이

  -> 깊어지면 더 복잡한 패턴 파악이 가능하지만 과적합 발생 우려

  * 최소 샘플 분할(min_samples_split)

  : 분할을 고려하기 위해 노드에 있어야 하는 최소 샘플 수

  -> 값이 높을수록 매우 특정한 패턴(과적합)을 학습하는 것을 방지,

    낮을수록 데이터에 노이즈 포착 가능성 우려

  * 최소 샘플 리프(min_samples_leaf)

    : 분할 후 리프 노드에 있어야 하는 최소 샘플 수 결정

    -> 위와 유사하나 값이 높을수록 샘플이 거의 없는 리프 노드 생성 X
  

  * 최대 기능(max_features)

    : 노드 분할을 위해 고려되는 최대 기능 수

    -> 이 수를 줄이면 앙상블 모델에서 트리의 다양성이 증가할 수 있지만

      중요 기능이 제외될 수도


  * 기준(기준)

    : 분할 품질을 측정하는데 사용되는 기능

    -> 일반적 기분에는 gini, 엔트로피, mse(평균 제곱 오류)

    -> 기준 선택은 나무가 자라는 방식에 영향, 모델 성능에 영향

  * 최대 리프 노드(max_leaf_nodes)

    : 트리의 최대 리프 노드 수


    -> 트리의 크기와 복잡성을 제어하는 데 사용, 리프 노드가 많을 수록 더 복잡한 모델 가



<장단점>

  장점 : 

  해석성 : 쉽게 이해하고 해석이 가능함
  데이터 정규화 필요 없음
  비선형 관계 처리 가능


  단점 :

  과적합 : 잡음이 있는 데이터를 과적합하는 경향 존재
  불안정성 : 데이터의 작은 변화로인해 완전히 다른 트리가 나타날 수도
  편향된 트리 : 지배적인 클래스에 편향되어 있음

=> 혼동행렬을 활용하여 성능 비교하는 방법

  행 : 실제 클래스

  열 : 예측된 클래스

1. 지니계수 활용했을 때

  DecisionTreeClassifier(criterion='gini')

  후 confusion

2. 엔트로피 활용

  DecisionTreeClassifier(criterion='entropy')

  후 confusion

3. 가지치기 작업

  DecisionTreeClassifier(criterion='gini', max_depth=3)

  : 학습된 나무에서 불필요한 부분 제거

  : 전방향, 후방향이 있음

    전방향의 대표적인 매개변수 : max_depth, min~split,leaf

  -> 과적합 방지에 도움

    일반화 성능 향상

    복잡도를 줄였기에 정확도가 낮을수

    의사 결정 나무는 높은 정확도를 달성할 수 있지만, 너무 복잡해지면 새 데이터에 대한 성능이 떨어질수도

    즉, 과적합 발생(학습 데이터의 노이즈까지 학습하여 새 데이터에서 성능이 떨어지는 현상)

=========================================================

** KNN

  : 가장 속성이 비슷한 이웃을 먼저 찾음

    -> 가까운 목표 값과 같은 값으로 분류하여 예측

    -> k값에 따라 예측의 정확도가 달라지므로 적절한 k값을 찾아야함

      KNeighborsClassifier(n_neighbors=k) - default 5

    **** k가 짝수일 경우 동점이 발생할 수 있으니 되도록 홀수

        짝수일 땐 거리순으로

    **** predict_proba() 메서드 지원X


=========================================================

** SVM(서포트 벡터머신)

  : 데이터를 고차원 공간으로 변환하여 서로 다른 클래스 간의 최대 마진 찾음

  : 두 개의 클래스로 나누는 결정 경계(결정 초평면)을 찾아 가장 가까운 훈련샘플(서포트 벡터)

    까지의 거리가 최대가 되는 선을 찾는 것을 목표 == 마진 최대화


  -> 비선형 분류 문제에도 사용 가능(커널트릭을 통해 고차원 공간으로 변환 후 선형 결정 경계 찾음)


  -> 높은 차원의 데이터에도 가능하지만 데이터 셋이 크거나 노이즈가 많거나 선형적으로 구분되지 않은 경우엔

    성능이 떨어질 우려


    *** 결정경계는 선이 아닌 평면

      : 시각적으로 인지 가능한건 3차원까지. 그 이상은 초평면이라고 부름


  <kernel>

1. 선형 커널 (Linear Kernel)

  : 간단하고 빠르며, 데이터가 선형적으로 구분될 때 좋은 성능을 보임

  -> 일부 클래스 간의 분류가 충분하지 않을 수도

2. 다항식 커널 (Polynomial Kernel)

  : 비선형적으로 분포된 데이터를 더 잘 처리할 수 있음

  -> 차수가 증가함에 따라 모델이 더 복잡해지고,

   데이터의 패턴을 더 잘 포착할 수 있지만, 과적합의 위험이 있음

3. RBF 커널 (RBF Kernel)

  : 비선형적인 경계를 가진 데이터에서 가장 효과적

  -> Wine 데이터셋의 복잡한 패턴을 잘 포착할 수 있으며, 높은 성능을 기대할 수 있음






  <C 매개변수>

  :  SVM 분류기의 정규화 매개변수

  -> 모델이 각각의 데이터 포인트들을 얼마나 정확하게 분류할 것인지를 결정

  -> C값이 높을수록 데이터를 더 정확히 분류하려고 하여 결정경계가 개별 데이터 포인트에

    더 밀접하게 맞추어져 있는 상태 (과적합 우려)

  -> 낮으면 덜 민감하여 잘못 분류할 가능성이 더 발생하지만 일반화된 모델을 더 잘 만들 수도


    *** 1.0 기본값 : 상대적으로 균형 잡힌 정규화 수준 제공



=========================================================

앙상블 학습

: 서로 다른 또는 같은 알고리즘 결합

  -> 여러개의 약한 학습기를 결합해 확률적인 보완 수행하고

  오류가 발생하는 부분에 가중치를 업데이트해가며 예측 성능 높임

유형

  1. 보팅

  : 서로 다른 알고리즘을 가진 분류기 결합

  1) 하드 보팅: 다수결 원칙

  2) 소프트 보팅: 결정 확률을 모두 더해 평균하고
  
  이들 중 가장 높은 레이블 값을 최종 보팅 결과값으로 선정


  2. 배깅

  : 각각의 분류기가 모두 같은 유형의 결합

  ex) random forest

  3. 부스팅


=========================================================

Random Forest

여러 개의 의사결정 트리(Decision Trees)를 조합하여 작동하는 앙상블 학습 방법의 하나

다수의 의사결정 트리를 생성하고, 각각의 트리가 데이터에 대한 예측을 수행한 후,

이 예측들을 결합하여 최종 예측 결과를 도출


베깅은 원본 훈련 데이터 세트에서 복원 추출 방식(하나의 데이터 포인트를 여러 번 샘플링할 수 있음)을 사용하여

여러 개의 서브셋(부트스트랩 샘플)을 생성

각 부트스트랩 샘플은 원본 데이터 세트와 크기는 같지만, 일부 데이터는 중복되고 일부 데이터는 누락될 수

배깅 장점 :

  분산 감소 : 여러 모델 예측 결합함으로써 개별 모델이 가질 수 있는 높은 분산 줄일수

  오버피팅 방지 : 부트스트랩 샘플링으로 인해 각 모델이 훈련 데이터의 다른 측면을 학습하므로

  병렬 처리 가능 : 각 모델은 독립적으로 훈련되므로 병렬 처리 가능해져 계산 효율성이 높음



