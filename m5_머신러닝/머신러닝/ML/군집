
비지도학습




1. 차원 축소

2. 군집


-> 라벨링이 되지 않은 데이터가 많을 때 비슷한 특징이나 패턴을 가진 데이터들끼리
    군집화한 후, 새로운 데이터가 어떤 군집에 속하는지를 추론


=> 생성된 군집 번호는 데이터 내에서 자동으로 탐지된 패턴이나 구조를 반영한 것이지

실제 타겟 값을 반영한 건 X


1) K-평균 KMeans

거리 기반으로 군집 중심점을 이동시키면서 군집화 수행

2) 평균이동

K-평균과 유사하나 데이터가 모여있는 밀도가 가장 높은 쪽으로 군집 중심점을 이동하면서

군집화를 수행


-> 컴퓨터 비전 영역에서 이미지나 영상 데이터의 특정 개체 구분에 적합한 알고리즘

3) GMM

군집화를 적용하고자 하는 데이터를 여러 개의 가우시안 분포모델을 섞어서

생성된 모델로 가정해 수행하는 방식

-> k-평균보다 유연하고 다양한 데이터 셋에 적용 가능하지만 수행시간이 오래 걸림


4) DBSCAN

밀도 기반 군집화의 대표적인 알고리즘으로 데이터의 분포가 기하학적으로 복잡한

데이터 셋에 효과적인 군집화가 가능함


* kmeans의 객체 속성

labels_ 
: 각 데이터 포인트가 속하는 클러스터의 레이블

cluster_centers_
: 각 클러스터의 중심 좌표

inertia_
: 클러스터 내의 데이터 포인트들 간의 거리의 합


n_iter_
: 알고리즘이 수렴하는 데 걸린 반복 횟수

** 군집 평가

실루엣 분석으로 평가

=> 얼마나 효율적으로 분리되어있는지

=> -1 ~ 1사이 0으로 가까울 수록 근처의 군집과 가깝다는 것


- 실루엣 계수 

s(i) = (b(i) - a(i))/max(a(i),b(i))

a(i)
: 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값

b(i)
: 해당 데이터 포인트가 속하지 않은 군집 중 가장 가까운 군집과의 평균 거리

silhouette_samples, silhouette_score 이용

1) silhouette_samples
: 인자로 X feature 데이터 세트와 각 피처 데이터 세트가 속한 군집 레이블 값인
  labels 데이터를 입력해 주면 각 데이터 포인트의 실루엣 계수를 반환

2) silhouette_score
: 인자로 X feature 데이터 세트와 각 피처 데이터 세트가 속한 군집 레이블 값인
  labels 데이터를 입력해 주면 전체 데이터의 실루엣 계수 값을 평균해 반환

  -> np.mean(silhouette_samples())와 같음

  => 값이 높을수록 군집화가 잘 되었다고 볼 수 있지만

  개별 군집의 평균값의 편차가 크지 않아야함

*** 해석시 주의해야할 요소 ***


1. 실루엣 계수 값

  평균 실루엣 스코어: 모든 데이터 포인트의 실루엣 스코어의 평균값이 높을수록
    클러스터링 품질이 좋다고 할 수있음

2. 실루엣 다이어그램의 형태

  너비 : 데이터 포인터가 얼마나 그 클러스터에 잘 속해있는지

  높이 : 데이터 포인트의 수



3. 실루엣 다이어그램의 일관성

  일관된 너비와 높이: 모든 클러스터의 실루엣 계수가 1에 가깝고,
    너비와 높이가 비교적 균일하다면, 이는 클러스터링이 잘 되었음을 의미

  불균일한 너비 또는 음수 값 : 잘못된 클러스터 할당이나,
    적절하지 않은 클러스터 수를 선택했을 가능성을 시사


=> 적당한 너비의 균일한 다이어그램들이면 좋겠지만

  a,b중 a의 높이가 월등히 큰 것과

  a,b,c,d의 너비와 높이가 비교적 균일 할 때

  클러스터 수를 정하는건 도메인 지식을 활용..

