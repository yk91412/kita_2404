{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEcleEPpxgJARS/nn22kT3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XIB4YKDw8Ffx"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","\n","card_df = pd.read_csv('/content/drive/MyDrive/kdt_240424/m5_머신러닝/dataset/creditcard.csv')\n","card_df.head(3)"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환\n","def get_preprocessed_df(df=None):\n","    df_copy = df.copy()\n","    df_copy.drop('Time', axis=1, inplace=True)\n","    return df_copy"],"metadata":{"id":"-a9T_Hc48K5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수.\n","# 데이터를 전처리하는 get_preprocessed_df 함수\n","def get_train_test_dataset(df=None):\n","    # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환\n","    df_copy = get_preprocessed_df(df)\n","    # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들\n","    X_features = df_copy.iloc[:, :-1]\n","    y_target = df_copy.iloc[:, -1]\n","    # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할\n","    X_train, X_test, y_train, y_test = \\\n","    train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)\n","    # 학습과 테스트 데이터 세트 반환\n","    return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)"],"metadata":{"id":"U6EpXnAK8MH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train.value_counts()"],"metadata":{"id":"aIwaWs8E8Szc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('학습 데이터 레이블 값 비율')\n","print(y_train.value_counts()/y_train.shape[0] * 100)\n","print('테스트 데이터 레이블 값 비율')\n","print(y_test.value_counts()/y_test.shape[0] * 100)"],"metadata":{"id":"VGXjrRBb8Ocx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import roc_auc_score\n","\n","def get_clf_eval(y_test, pred=None, pred_proba=None):\n","    confusion = confusion_matrix( y_test, pred)\n","    accuracy = accuracy_score(y_test , pred)\n","    precision = precision_score(y_test , pred)\n","    recall = recall_score(y_test , pred)\n","    f1 = f1_score(y_test,pred)\n","    # ROC-AUC 추가\n","    roc_auc = roc_auc_score(y_test, pred_proba)\n","    print('오차 행렬')\n","    print(confusion)\n","    # ROC-AUC print 추가\n","    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n","    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"],"metadata":{"id":"Jvu3Ecx98TMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","# 데이터 스케일링\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","lr_clf = LogisticRegression(max_iter=100, solver='lbfgs', random_state=42)\n","lr_clf.fit(X_train, y_train)\n","lr_pred = lr_clf.predict(X_test)\n","lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n","\n","# 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행.\n","get_clf_eval(y_test, lr_pred, lr_pred_proba)"],"metadata":{"id":"QJ47nIL18VX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인자로 사이킷런의 Estimator객체와, 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행.\n","def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n","    model.fit(ftr_train, tgt_train)\n","    pred = model.predict(ftr_test)\n","    pred_proba = model.predict_proba(ftr_test)[:, 1]\n","    get_clf_eval(tgt_test, pred, pred_proba)"],"metadata":{"id":"1lkjv3C68XWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightgbm import LGBMClassifier\n","\n","lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n","get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"],"metadata":{"id":"4XMUboDNDXHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 신용카드 거래 금액('Amount')의 히스토그램을 생성\n","import seaborn as sns\n","\n","plt.figure(figsize=(8, 4))\n","plt.xticks(range(0, 30000, 1000), rotation=60)\n","sns.histplot(card_df['Amount'], bins=100, kde=True)\n","plt.show()"],"metadata":{"id":"wnVuPZsCDuFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","# 사이킷런의 StandardScaler를 이용하여 정규분포 형태로 Amount 피처값 변환하는 로직으로 수정.\n","def get_preprocessed_df(df=None):\n","    df_copy = df.copy()\n","    scaler = StandardScaler()\n","    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))\n","    # 변환된 Amount를 Amount_Scaled로 피처명 변경후 DataFrame맨 앞 컬럼으로 입력\n","    df_copy.insert(0, 'Amount_Scaled', amount_n)\n","    # 기존 Time, Amount 피처 삭제\n","    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n","    return df_copy"],"metadata":{"id":"MvLXkewNDu1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행.\n","X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n","\n","print('### 로지스틱 회귀 예측 성능 ###')\n","lr_clf = LogisticRegression()\n","get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n","\n","print('### LightGBM 예측 성능 ###')\n","lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n","get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"],"metadata":{"id":"tuv3ITGbDwOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터의 상관 관계를 시각화 : v14, v17이 class와 상관관계가 높음\n","import seaborn as sns\n","\n","plt.figure(figsize=(9, 9))\n","corr = card_df.corr()\n","sns.heatmap(corr, cmap='RdBu')"],"metadata":{"id":"OFREKTC0EnJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터에서 이상치를 찾는 get_outlier 함수를 정의\n","import numpy as np\n","\n","def get_outlier(df=None, column=None, weight=1.5):\n","    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함.\n","    fraud = df[df['Class']==1][column]\n","    quantile_25 = np.percentile(fraud.values, 25)\n","    quantile_75 = np.percentile(fraud.values, 75)\n","    # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함.\n","    iqr = quantile_75 - quantile_25\n","    iqr_weight = iqr * weight\n","    lowest_val = quantile_25 - iqr_weight\n","    highest_val = quantile_75 + iqr_weight\n","    # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환.\n","    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n","    return outlier_index"],"metadata":{"id":"7EdIZZy-En7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get_outlier 함수를 호출하여 'V14' 피처의 이상치 인덱스를 찾고, 이를 출력\n","outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)\n","print('이상치 데이터 인덱스:', outlier_index)"],"metadata":{"id":"duxiiQGTE4ln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get_processed_df( )를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경.\n","def get_preprocessed_df(df=None):\n","    df_copy = df.copy()\n","    amount_n = np.log1p(df_copy['Amount'])\n","    df_copy.insert(0, 'Amount_Scaled', amount_n)\n","    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n","    # 이상치 데이터 삭제하는 로직 추가\n","    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n","    df_copy.drop(outlier_index, axis=0, inplace=True)\n","    return df_copy\n","\n","X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n","print('### 로지스틱 회귀 예측 성능 ###')\n","get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n","print('### LightGBM 예측 성능 ###')\n","get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"],"metadata":{"id":"YCPkLNS4E5E6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.log1p 함수를 사용하여 'Amount' 피처를 로그 변환\n","def get_preprocessed_df(df=None):\n","    df_copy = df.copy()\n","    # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환\n","    amount_n = np.log1p(df_copy['Amount'])\n","    df_copy.insert(0, 'Amount_Scaled', amount_n)\n","    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n","    return df_copy"],"metadata":{"id":"hUMhmyGkFxo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이 셀에서는 로그 변환된 'Amount' 피처를 사용하여 로지스틱 회귀와 LightGBM 모델을 다시 학습하고 예측 성능을 평가\n","X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n","\n","print('### 로지스틱 회귀 예측 성능 ###')\n","get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n","\n","print('### LightGBM 예측 성능 ###')\n","get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"],"metadata":{"id":"eWLDNE24GNJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불균형한 데이터셋 처리를 위한 imbalanced-learn 라이브러리를 설치\n","! pip install imbalanced-learn"],"metadata":{"id":"DrYpNlcRFVIV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SMOTE(Synthetic Minority Over-sampling Technique)를 사용하여 불균형한 데이터셋을 처리\n","from imblearn.over_sampling import SMOTE\n","\n","smote = SMOTE(random_state=0)\n","X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n","print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n","print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n","print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_train_over).value_counts())"],"metadata":{"id":"ki5ddT2jFU4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  SMOTE를 적용한 학습 데이터셋을 사용하여 로지스틱 회귀 모델을 학습하고 예측 성능을 평가\n","lr_clf = LogisticRegression()\n","# ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의\n","get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)"],"metadata":{"id":"QbimH7jmGyGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Precision-Recall 커브를 시각화하는 함수\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.metrics import precision_recall_curve\n","\n","def precision_recall_curve_plot(y_test , pred_proba_c1):\n","    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n","    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n","\n","    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n","    plt.figure(figsize=(8,6))\n","    threshold_boundary = thresholds.shape[0]\n","    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n","    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n","\n","    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n","    start, end = plt.xlim()\n","    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n","\n","    # x축, y축 label과 legend, 그리고 grid 설정\n","    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n","    plt.legend(); plt.grid()\n","    plt.show()"],"metadata":{"id":"AArm3WFHG0T9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SMOTE(Synthetic Minority Over-sampling Technique)를 적용한 후에 정밀도가 낮고 재현율이 높은 문제가 발생할 수 있습니다. SMOTE는 소수 클래스 데이터를 인위적으로 생성하여 클래스 불균형을 해결하는 기법입니다. 그러나 이로 인해 모델이 양성(Positive) 클래스를 과도하게 예측하게 되는 경우가 생길 수 있습니다. 이는 많은 거짓 양성(False Positive)을 초래할 수 있으며, 결과적으로 정밀도가 낮아질 수 있습니다.\n"],"metadata":{"id":"ipiui7vGG1c-"}},{"cell_type":"code","source":["precision_recall_curve_plot( y_test, lr_clf.predict_proba(X_test)[:, 1] )"],"metadata":{"id":"5_wBD3-NIO23"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LightGBM이 SMOTE로 증강된 데이터를 사용하여 모델링할 때 정밀도가 정상이 되는 이유는 다음과 같습니다:\n","- LightGBM의 강력한 성능: 데이터 처리와 피처 중요도를 잘 반영하는 학습 방식.\n","- SMOTE와 LightGBM의 시너지 효과: 클래스 불균형 문제를 해결하여 균형 잡힌 모델 학습.\n","- 모델 튜닝 및 하이퍼파라미터 최적화: 적절한 하이퍼파라미터 설정을 통해 모델 성능 향상"],"metadata":{"id":"NfC9kK9hHT8F"}},{"cell_type":"code","source":["lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n","get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test,\n","                  tgt_train=y_train_over, tgt_test=y_test)"],"metadata":{"id":"Q8uQuLK4HUap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S3-oi17OHVrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2OK1rrAEHWMg"},"execution_count":null,"outputs":[]}]}