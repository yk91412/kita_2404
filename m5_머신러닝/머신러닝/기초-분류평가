
머신러닝 - 지도학습

<데이터셋에 정답이 있는 경우>

1. 분류 모델

  예측하고자 하는 값이(종속변수) 범주형일 때 주로 사용

  이진 / 다중 분류


  TP, FP, FN, TN (실제 클래스 값, 예측 클래스 값)

  TP : 양성(positive 값 1)으로 예측했고, 그게 맞음 => 양성
  FP : 양성으로 예측했으나 틀림 => 즉 음성
  FN : 음성(negative 값 0)으로 예측했으나 틀림 => 즉 양성
  TN : 음성으로 예측했고, 그게 맞음 => 음성


<평가 지표>

  1. 정확도

  : 모델이 전체 데이터 중 얼마나 정확히 예측했는가

  가장 직관적인 분류 성능 평가 지표

  (TP+TN) / (TP+TN+FN+TN)

    ** 예측이 얼마나 잘 맞았는가(전체 예측)



  2. 정밀도

    : 모델이 양성이라고 예측한 것 중 실제 양성인 비율

    TP / (TP+FP)


    ** 양성이라고 예측한 것중에 얼마나 잘 맞았는가 (양성의 예측)



  3. 재현율

    : 실제 양성인 데이터 중 모델이 양성으로 올바르게 예측한 비율


    TP / (TP+FN)


      ** 양성인 데이터를 얼마나 예측을 잘 했나


  [정밀도 / 재현율 trade-off관계]

  임곗값이 낮아질수록 (양성 예측값이 많아질수록)

  FN은 하락(분모가 작아진다) => 재현율은 올라감

  FP 증가(분모가 커진다) => 정밀도 내려감



  * 재현율이 더 중요한 경우 : 

    실제 양성을 음성으로 판단하게 되면 크게 영향이 되는 보험사기, 암진단

  * 정밀도가 더 중요한 경우:

    스팸 메일이 아닌데 스팸 메일로 분류하여 업무 차질 발생


  => 적절한 임곗값 필요


  4. f1 score

    : 모델이 양성 데이터를 얼마나 잘 예측하는지에 대한 전반적인 측정치

    정밀도와 재현율의 조화 평균

    불균형 데이터 셋에서 유용

      
      2 * (precision * recall) / (precision + recall)

  -> 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치일 때 높은 값


  5. ROC AUC

    ROC 곡선 : 모델이 양성 클래스를 어떻게 잘 예측하는지에 대한 성능 그래프

      => 재현율과 1-특이성의 관계를 보여줌


    [특이성]

    실제로 음성인 사례들 중에서 모델이 음성으로 예측한 비율


    TN / (TN+FP)



    AUC값은 ROC 곡선 아래의 면적, 값이 클수록 모델의 성능이 좋다(0~1)


============================================================


  predict_proba() 

  : 개별 레이블별 결정 확률


  <predict와 차이점>

  샘플 데이터가 [0.3, 0.7] 일때

  predict는 0일 확률 0.3 1일 확률 0.7로 봐서 1을 반환

  predict_proba는 개별 확률을 나타내므로 0일 확률 0.3, 1일 확률 0.7이니깐

  [0.3, 0.7]로 반환



============================================================


Binarizer

  : 임계값(threshold)을 기준으로 이진값(0 또는 1)으로 변환

  ex) binarizer = Binarizer(threshold=1.1)

    binarizer.fit_transform(X)


[transform vs fit_transform]

fit_transform : 데이터 변환(전처리)을 위해 주어진 데이터에 대해 모델을 학습하고 변환을 동시에 수행


=> 특히 학습 데이터에 대해 통계적인 정보를 추출한 후,

    이를 기반으로 변환을 수행하는 경우에 많이 사용


transform : 이미 학습된 모델이나 전처리기를 사용하여 주어진 데이터를 변환

=> 학습된 모델의 파라미터를 새로운 데이터에 적용


ex) scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)


============================================================


혼동 행렬(confusion matrix) == 오차행렬

: 머신러닝 및 통계에서 분류 모델의 성능을 평가하는 데 사용되는 행렬

    모델이 예측한 결과와 실제 결과 간의 관계를 나타냄

TN FP
FN TP


============================================================


precision_recall_curve


 : Precision-Recall 곡선을 생성하기 위해 사용되는 함수

  Precision-Recall 곡선: Precision과 Recall 간의 trade-off 관계를 시각화한 그래프

  => 분류 모델의 성능을 평가하는 데 널리 사용되며,
  주로 불균형 데이터셋에서 모델의 성능을 평가하는 데 유용


============================================================

ROC 곡선과 AUC

: 수신자 판단 곡선

이진 분류 모델의 예측 성능을 판단하는 중요한 평가 지표

FPR이 변할 때 TPR이 어떻게 변하는지 나타내는 곡선

-> FPR을 0부터 1가지 변경하면서 그 변화에 따른 TPR의 변화값을 구함

=> AUC 수치가 커지려면 FPR이 작은 상태에서 얼마나 큰 TPR을 얻을 수 있는지가 관건



FPR = FP/(TN+FP) = 1- 특이성 = (TN+FP) /(TN+FP) - TN/(TN+FP)

=> 음성인 사례 중 예측을 잘못한 비율

TPR 재현율

TNR = TN/(TN+FP)

=> 음성인 사례 중 음성으로 예측한 비율



roc_curve

실제값 (y_true)과 예측 확률 또는 점수 (y_score)를 입력으로 받아

 Positive 샘플의 True Positive Rate (TPR)과

  실제 Negative 샘플의 False Positive Rate (FPR)을 계산하여 반환



============================================================


label_binarize


: 다중 클래스 레이블을 이진 형식으로 변환하는 데 사용되는 함수

해당 클래스에 속하면 1로, 속하지 않으면 0으로 표시
