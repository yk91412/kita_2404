
** 머신러닝 **

 컴퓨터가 명시적으로 프로그래밍되지 않고도 데이터에서 학습하고 예측 가능


- 핵심사항

 데이터: 머신러닝은 데이터를 기반으로 학습. 즉 좋은 데이터가 좋은 모델의 핵심

 알고리즘: 머신러닝에는 다양한 알고리즘이 있습니다. 각 알고리즘은 특정 유형의 문제에 더 적합

 하이퍼파라미터: 알고리즘의 성능에 영향을 미치는 매개변수 / 실험을 통해 최적화

 평가 지표: 모델의 성능을 측정하는 방법

  1) 분류모델:

    정확도, 정밀도, 재현율,  F1 점수


  2) 회귀모델 :

    평균 절대 오차, 회귀 작업의 평균 제곱근 오차와 같은 메트릭을 포함


- 유형

  1) 지도 학습: 

  선형 회귀, 로지스틱 회귀, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅, SVM(Support Vector Machines) 및 k-NN(k-nearest neighbours)
  
  과 같은 알고리즘은 레이블이 지정된 학습 데이터를 사용하여

  입력 기능을 기반으로 대상 변수를 예측하는 데 사용

  (명확한 답이 있음)

  => 알려진 입력 데이터 세트(특징)와 데이터에 대한 알려진 응답(대상 또는 레이블)을 입력받아 모델을 훈련하여

     새 데이터에 대한 응답에 대한 합리적인 예측을 생성

  ** 두 가지 유형의 문제에 사용

  회귀:
  
  대상 또는 종속 변수가 연속적이거나 정렬된 전체 값일 때 사용
  
  회귀에 사용되는 알고리즘에는 선형 회귀, 결정 트리, 랜덤 포레스트 및 GBM 계열의 회귀트리 모델 포함.
  
  => ex) 주택 가격 예측
  
  -> mse(Mean Squared Error) 예측값과 실제값의 차이의 제곱의 평균 :
  
    값이 작을수록 모델의 예측이 정확함을 의미


  분류:
  
  분류는 대상 변수가 범주형일 때,
  
  간단히 말해서 입력 데이터를 범주로 분류할 때 사용
  
  분류에 사용되는 알고리즘에는 Logistic Regression, Naive Bayes, Decision Trees, Random Forests, Support Vector Machines
  
  및 Neural Networks 및 GBM 계열의 Boosting 모델 포함
  
  => ex) 붓꽃의 종류를 분류


 * 알고리즘이 작동하는 방식

  데이터 수집:
  
  데이터를 수집하고 전처리. 데이터는 feature와 label로 구분
  
  이를 위해 사용되는 데이터는 출력 데이터 세트도 제공되기 때문에 레이블이 지정된 데이터라고 한다
  
  모델 훈련:
  
  알고리즘은 훈련 데이터를 통해 학습. feature와 label 간의 패턴 또는 관계를 발견하려고 시도
  
  모델 예측:

  모델이 훈련되면 본 적이 없는 새로운 데이터의 결과를 예측하는 데 사용

  이 새 데이터에 대한 입력을 테스트 데이터라고 한다
  
  평가:

  모델의 예측을 실제 값과 비교하여 모델의 정확도를 평가
  
  정확도, 정밀도, 재현율, F1 점수(분류용), 평균 절대 오차(MAE), 평균 제곱 오차(MSE), 평균 제곱근 오차(RMSE)(회귀용)

  와 같은 다양한 메트릭이 모델을 평가하는 데 사용.
  
  조정:

  모델의 성능이 만족스럽지 않으면 모델로 돌아가 하이퍼파라미터를 조정하거나 다른 모델을 모두 선택 가능
  
  예측:
  
  만족스러운 성능이 달성되면 이제 모델을 사용하여 보이지 않는 새로운 데이터를 예측할 수 있다.


  2) 비지도 학습: 

  k-평균, 계층적 클러스터링 및 DBSCAN과 같은 클러스터링 기법 및 PCA(Principal Component Analysis), LDA, SVD 등의 차원 축소 기법
  (답은 없고 데이터만 있음)




  => 레이블이 지정되지 않은 데이터를 사용하여 모델을 학습하는 방법

      모델의 성능을 평가하기 위한 레이블이 불필요

      데이터를 이해하고 데이터에서 패턴을 찾는 데 사용


  ** 알고리즘  

    - 클러스터링: 클러스터링 알고리즘에는 K-평균, 평균-점 클러스터링, 밀도기반 군집(DBSCAN) 등

    - 차원 축소: 차원 축소 알고리즘에는 주성분 분석(PCA), 선형판별분석(LDA),   특이값 분해(SVD) 등
    


  3) 강화 학습:

  에이전트가 조치를 취하고 그 대가로 보상이나 처벌을 받음으로써 환경에서 행동하는 방법을 배우는 기계 학습으로

  목표는 시간이 지남에 따라 누적 보상을 최대화하는 것

  4) 신경망 및 딥러닝:

  신경망, 역전파 및 CNN(Convolutional Neural Networks), RNN(Recurrent Neural Networks),

  Long Short-Term 메모리(LSTM) 네트워크


  5) 자연어 :

  트랜스포머 기반의 BERT, GPT Series의 발전으로 자연어 처리의 급속한 발전 진행

  6) LLM(Large Language Model) :

  거대 생성 언어모델 기반의 ChatGPT 시리즈의 발전으로 인간과 상호작용할 수 있는 수준으로 발전

  7) 비전:

  Yolo를 이용한 다양한 객체탐지솔루션이 폭넓게 활용되고 Stable Diffusion이 이미지 생성을 위한 주요 솔루션으로 확대 예정  


============================================================


## 사이킷런

파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리

사이킷런은 파이썬 기반의 머신러닝을 위한 가장 쉽고 효율적인 개발 라이브러리를 제공

- 특징

파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스러운 API를 제공

머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공

많은 머신러닝 알고리즘이 효율적으로 구현되어 있기에 머신러닝을 처음 배울 때 사용하기 유용

사이킷런은 오픈 소스 라이브러리이기 때문에 누구나 자유롭게 사용 가능

=> 
  분류
  회귀
  클러스터링
  차원 축소
  특징 추출
  모델 선택 및 평가


다양한 프레임워크와 API를 제공

=> 
  데이터 전처리
  모델 학습 및 예측
  모델 저장 및 복원
  모델 모니터링
  모델 배포



주요 모듈

model_selection: 학습 데이터와 테스트 데이터를 분리하거나 교차 검증, 

 Estimator의 하이퍼 파라미터를 튜닝하기 위해서 다양한 함수와 클래스를 제공

preprocessing: 데이터를 정규화, 스케일링, 인코딩하는 등 다양한 기능을 제공

datasets: 다양한 데이터 세트를 제공

linear_model: 선형 회귀, 로지스틱 회귀, 릿지 회귀, 라쏘 회귀 등 다양한 선형 모델을 제공

tree: 의사 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리 등 다양한 트리 기반 모델을 제공

svm: 서포트 벡터 머신, 커널 서포트 벡터 머신 등 다양한 서포트 벡터 머신을 제공

neighbors: K-최근접 이웃 알고리즘을 제공

ensemble: 앙상블 방법을 제공

cluster: 클러스터링 알고리즘을 제공



** 데이터를 전달할 때 표준화하여 전달해줌

============================================================


로지스틱 회귀(Logistic Regression)
분류 문제를 해결하기 위한 알고리즘으로, 선형 회귀와 마찬가지로 입력 특성의 가중치 합을 계산하는데, 선형 회귀와 달리 결과를 이진 분류(0 또는 1, 참 또는 거짓 등)에 사용할 수 있는 확률로 변환.

로지스틱 회귀는 선형 회귀와 비슷하게 모델의 예측과 실제 값 사이의 차이를 최소화하도록 가중치를 학습. 하지만 로지스틱 회귀는 선형 회귀와는 달리 결과를 0과 1 사이의 값으로 제한하는 로지스틱 함수(또는 시그모이드 함수)를 사용.

로지스틱 회귀는 각 클래스에 속할 확률을 제공하며, 특정 임계값(일반적으로 0.5)을 초과하는 경우 데이터 포인트를 해당 클래스에 할당. 이는 이진 분류뿐만 아니라 다중 클래스 분류에도 적용될 수 있다(이 경우에는 일대다(OvR) 또는 다항 로지스틱 회귀를 사용할 수 있다).

로지스틱 회귀는 출력이 확률이기 때문에, 결과의 해석이 직관적이며 모델의 예측이 불확실한 경우에도 그 정도를 측정할 수 있다. 또한 로지스틱 회귀는 선형 회귀보다 이상치에 덜 민감하며, 모델이 과적합되는 것을 방지하기 위해 규제를 적용할 수 있다는 장점도 있다.

로지스틱 함수, 또는 시그모이드 함수는 S-자 형태를 띠는 함수로, 실수 입력값을 0과 1 사이의 출력값으로 변환하는 데 사용. 이 함수는 머신러닝, 특히 이진 분류 문제에서 중요한 역할을 한다.

로지스틱 함수의 정의

f(x) = 1 / (1 + e^-x)

e는 자연 상수(약 2.71828). x는 어떤 실수 값도 가능하며, -무한대에서 무한대까지의 범위를 가지며 이 함수는 모든 실수 입력에 대해 0과 1 사이의 값을 반환.

함수가 결과를 0과 1 사이로 제한하기 때문에, 이는 확률에 대해 논의할 때 특히 유용. 로지스틱 회귀 분석에서 이 함수는 선형 함수의 결과를 확률로 변환하는데 사용.

입력값 x가 커질수록 로지스틱 함수의 출력은 1에 가까워지고, x가 작아질수록 출력은 0에 가까워진다. x가 0일 때 로지스틱 함수의 값은 0.5입니다. 이러한 특성 때문에 로지스틱 함수는 이진 분류 문제에 널리 사용.


============================================================


교차 검증
모델의 성능을 평가하기 위한 기법으로, 데이터를 여러 번 나누어 여러 모델을 학습시키고 검증하는 방법입니다. 교차 검증을 통해 모델의 일반화 성능을 더 잘 추정할 수 있습니다.

교차 검증의 과정

데이터 분할:
전체 데이터를 여러 개의 폴드(fold)로 나눕니다. 예를 들어, 5-폴드 교차 검증에서는 데이터를 5개의 폴드로 나눕니다.
모델 학습 및 평가:
각 폴드에 대해 한 번씩 테스트 데이터로 사용하고 나머지 폴드는 학습 데이터로 사용합니다.
즉, 5-폴드 교차 검증에서는 5번의 학습 및 평가 과정을 거칩니다.
성능 측정:
각 폴드에서 얻은 평가 점수를 기록하고, 이를 평균하여 최종 성능을 측정합니다.
장점
일반화 성능 추정: 모델이 새로운 데이터에 대해 얼마나 잘 작동할지를 더 정확하게 추정할 수 있습니다.
데이터 효율성: 데이터를 반복해서 사용하므로 데이터 낭비가 적습니다.
모델 튜닝: 모델의 하이퍼파라미터를 튜닝할 때 유용합니다.


============================================================

서포트 벡터 머신(Support Vector Machines, SVM)
분류나 회귀, 이상치 탐지 등에 사용되는 강력한 머신러닝 알고리즘 중 하나입니다. SVM은 주로 분류 문제에 사용되며, 이 알고리즘의 핵심 아이디어는 데이터를 고차원 공간으로 변환하여 서로 다른 클래스 간의 최대 마진을 찾는 것입니다.

SVM은 데이터를 두 개의 클래스로 나누는 결정 경계(결정 초평면이라고도 함)를 찾습니다. 이 결정 경계는 각 클래스의 가장 가까운 훈련 샘플(서포트 벡터라고 함)까지의 거리가 최대가 되는 선을 찾는 것을 목표로 합니다. 이를 '마진 최대화'라고 하며, 이 마진 최대화는 오류를 최소화하고 모델의 일반화 성능을 향상시키는 데 중요한 역할을 합니다.

SVM은 선형 뿐만 아니라 비선형 분류 문제에도 사용할 수 있습니다. 비선형 문제를 해결하기 위해, SVM은 커널 트릭이라는 기법을 사용하여 데이터를 고차원 공간으로 변환하고, 그 고차원에서 선형 결정 경계를 찾습니다. 이 커널 트릭 덕분에 SVM은 복잡한 분류 문제를 처리할 수 있습니다.

SVM은 작은 데이터셋에서도 잘 작동하며, 높은 차원의 데이터에 대해 강력한 성능을 발휘합니다. 그러나 데이터셋이 크거나 노이즈가 많은 경우, 그리고 데이터가 선형적으로 구분되지 않는 경우에는 다른 알고리즘(예: 랜덤 포레스트나 신경망)에 비해 성능이 떨어질 수 있습니다.


============================================================


그리드 서치를 통한 하이퍼파라미터 튜닝
GridSearchCV는 사이킷런에서 제공하는 하이퍼파라미터 튜닝 기법으로, 모델의 성능을 최적화하기 위해 다양한 하이퍼파라미터 조합을 시도하는 방법입니다. GridSearchCV는 사용자가 제공한 하이퍼파라미터 값들의 조합을 모두 시도하여, 가장 좋은 성능을 내는 조합을 찾습니다

GridSearchCV의 과정

하이퍼파라미터 그리드 설정: 튜닝할 하이퍼파라미터와 그 후보 값들의 집합을 정의합니다.
모델 초기화: 튜닝할 머신러닝 모델을 초기화합니다.
교차 검증을 통한 모델 학습 및 평가:
각 하이퍼파라미터 조합에 대해 교차 검증을 수행합니다.
교차 검증을 통해 얻은 평균 성능 점수를 기록합니다.
최적의 하이퍼파라미터 선택: 교차 검증 결과를 바탕으로 가장 높은 성능을 보인 하이퍼파라미터 조합을 선택합니다.
최적 모델로 학습: 최적의 하이퍼파라미터를 사용하여 모델을 학습시킵니다.
GridSearchCV의 장점

자동화된 하이퍼파라미터 튜닝: 다양한 하이퍼파라미터 조합을 자동으로 시도하여 최적의 조합을 찾을 수 있습니다.
교차 검증 사용: 교차 검증을 통해 모델의 일반화 성능을 평가하므로, 데이터셋에 대한 과적합을 방지합니다.
다양한 모델 지원: 사이킷런의 다양한 모델과 함께 사용할 수 있습니다


파라미터 그리드 설정: param_grid 변수에 SVM(Support Vector Machine) 모델의 하이퍼파라미터 C와 kernel의 후보 값들을 정의합니다.

C: 규제 파라미터로, 값이 작을수록 규제가 강해집니다.
kernel: SVM에서 사용할 커널 함수로, 여기서는 'linear'와 'rbf'를 시도합니다.
모델 초기화: SVC 모델을 초기화합니다.

그리드 서치 초기화 및 학습:

GridSearchCV 객체를 초기화하고, svc 모델과 param_grid를 전달합니다.
cv=5는 5-폴드 교차 검증을 의미합니다.
fit 메서드를 호출하여 그리드 서치를 수행하고 모델을 학습시킵니다.
최적 파라미터와 점수 출력:

grid_search.best_params_를 사용하여 최적의 하이퍼파라미터 조합을 출력합니다.
grid_search.best_score_를 사용하여 최적 조합에서의 교차 검증 평균 점수를 출력합니다.









============================================================


X : 입력 데이터 셋

=> ex) 주택 가격을 예측하는 경우 주택의 여러 특성(면적, 방의 수, 위치 등)을 포함한 데이터

y : 출력 데이터 또는 레이블

=> ex) 각 주택의 가격을 나타내는 값


 X_train은 훈련 데이터의 입력 특성을, y_train은 훈련 데이터의 실제 출력값(정답)

train_test_split : 학습 데이터와 테스트 데이터로 분리

test_size : 0.2~0.3을 가장 많이 사용

=> 0.2는 테스트에 20% 사용한다는 뜻


random_state : 임의의 수

데이터의 분할: train_test_split() 함수를 사용하여 데이터를 훈련 세트와 테스트 세트로 나눌 때,
데이터가 무작위로 섞이기 때문에 각 세트에 포함되는 데이터의 구성이 달라질 수 있습니다.
이는 각 실행에서 훈련 데이터와 테스트 데이터의 성격이 약간씩 달라질 수 있음을 의미합니다.

데이터의 크기: 데이터셋의 크기가 작거나 특정 패턴이나 극단적인 샘플이 포함되어 있을 경우,
다른 데이터셋 구성으로 인해 모델의 성능이 달라질 수 있습니다.

모델의 초기화: 일부 모델은 초기화 과정에서 난수를 사용하여 가중치를 초기화하거나 데이터를 분할하는 등의 과정에서
랜덤 요소를 포함할 수 있습니다. 이러한 경우에도 동일한 코드와 설정을 사용하더라도 각 실행에서 결과가 달라질 수 있습니다.


모델 초기화

변수 = DecisionTreeClassifier()

LinearRegression()

학습

초기화를 받은 변수.fit(X_train, y_train)

예측

y_pred = 초기화를 받은 변수.predict(X_test)


정확도 평가 (분류)

accuracy_score(y_test, y_pred)

모델 평가 (회귀)

mean_squared_error(y_test, y_pred)

============================================================



표준화

StandardScaler()



정규화

MinMaxScaler()


fit_transform

