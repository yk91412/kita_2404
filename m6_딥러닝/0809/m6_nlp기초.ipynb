{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM89O6jFN/OZGm9W547PBbQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 텍스트의 토큰화\n","**자연어 처리(NLP)에서 토큰화(Tokenization)**\n","- 텍스트 분석, 언어 번역, 감정 분석 등 다양한 NLP 작업을 위해 텍스트를 준비하는 기본 단계입니다.\n","- 토큰화는 텍스트를 단어, 문구, 기호 또는 기타 의미 있는 요소(토큰)로 분해하는 작업을 포함합니다. 이 과정에서 생성된 토큰은 추가 처리 및 분석을 위한 기본 구성 요소가 됩니다.\n","\n","토큰화의 목적\n","- 토큰화의 주요 목적은 텍스트 데이터를 단순화하여 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것입니다. 이를 통해 텍스트의 복잡성을 줄이고 일관성을 유지함으로써, 다양한 NLP 작업에서 효율적인 분석과 처리가 가능해집니다.\n","\n","토큰화의 유형\n","- 토큰화는 다양한 수준에서 수행될 수 있으며, 각 유형은 특정 NLP 작업의 요구 사항에 따라 선택됩니다:\n","  - 단어 토큰화 (Word Tokenization):\n","    텍스트를 개별 단어로 분해합니다.\n","    예: \"ChatGPT is amazing!\" → [\"ChatGPT\", \"is\", \"amazing\", \"!\"]\n","  - 문장 토큰화 (Sentence Tokenization):\n","    텍스트를 개별 문장으로 분해합니다.\n","    예: \"Hello world. How are you?\" → [\"Hello world.\", \"How are you?\"]\n","  - 하위 단어 토큰화 (Subword Tokenization):\n","    단어를 더 작은 의미 단위로 분해합니다. 주로 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 사용합니다.\n","    예: \"unhappiness\" → [\"un\", \"hap\", \"pi\", \"ness\"]\n","  - 문자 토큰화 (Character Tokenization):\n","    텍스트를 개별 문자로 분해합니다.\n","    예: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n","\n","토큰화의 과정\n","- 토큰화 과정은 일반적으로 다음 단계로 구성됩니다:\n","  - 텍스트 정규화 (Text Normalization):\n","    모든 텍스트를 소문자로 변환하여 일관성을 유지합니다.\n","    불필요한 구두점과 공백을 제거합니다.\n","    예: \"Hello, World!\" → \"hello world\"\n","  - 구분자 사용 (Delimiter-based Tokenization):\n","    공백이나 구두점을 기준으로 텍스트를 분해합니다.\n","    예: \"hello world\" → [\"hello\", \"world\"]\n","  - 고급 토큰화 기법 (Advanced Tokenization Techniques):\n","    언어의 문법적, 의미적 구조를 고려하여 토큰을 생성합니다.\n","    BPE, WordPiece, SentencePiece 등의 알고리즘을 사용합니다.\n","\n","토큰화의 중요성\n","- 토큰화는 NLP 작업에서 매우 중요한 역할을 합니다. 잘못된 토큰화는 후속 처리와 분석의 정확도에 큰 영향을 미칠 수 있습니다.\n","- 반면, 올바른 토큰화는 텍스트 데이터를 효과적으로 전처리하고 분석할 수 있게 합니다."],"metadata":{"id":"k6vXLxBB8Z3h"}},{"cell_type":"markdown","source":["## 고급 토큰화 기법\n","언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.\n","\n","\n","형태소 분석 (Morphological Analysis):\n","- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.\n","- 예를 들어, \"cats\"는 \"cat\"과 복수형 접미사 \"s\"로 분해됩니다.\n","- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.\n","\n","어간 추출 (Stemming):\n","- 단어의 어간을 추출하여 형태를 단순화합니다.\n","- 예: \"running\", \"runs\", \"ran\" → \"run\"\n","- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.\n","\n","어근 추출 (Lemmatization):\n","- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.\n","- 예: \"running\", \"ran\" → \"run\"\n","- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, \"better\"는 어근 \"good\"으로 변환됩니다.\n","- WordNetLemmatizer와 같은 도구가 사용됩니다.\n","\n","BPE (Byte Pair Encoding):\n","- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.\n","- 예: \"lowest\"가 \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"로 분해되고, 자주 등장하는 \"lo\", \"we\"가 결합되어 \"low\", \"est\"로 변환됩니다.\n","- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.\n","\n","WordPiece:\n","- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.\n","- 예: \"unhappiness\" → [\"un\", \"##happiness\"]\n","- 트랜스포머 모델(BERT 등)에서 사용됩니다.\n","\n","SentencePiece:\n","- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.\n","- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.\n","- 예: \"unhappiness\" → [\"un\", \"ha\", \"ppiness\"]\n","- Google의 T5, ALBERT 모델에서 사용됩니다."],"metadata":{"id":"e0_EHWPV-CcD"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpOf9M-Y8T34","executionInfo":{"status":"ok","timestamp":1723087067056,"user_tz":-540,"elapsed":377,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"7630294c-b5e9-4572-bff2-cf407a00ff6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","원문: \n"," 해보지 않으면 해낼 수 없다\n","\n","토큰화:\n"," ['해보지', '않으면', '해낼', '수', '없다']\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n","# 텍스트 전처리와 관련한 함수 중  text_to_word_sequence를 부러오기\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","text = '해보지 않으면 해낼 수 없다'\n","\n","result = text_to_word_sequence(text)\n","print('\\n원문: \\n',text)\n","print('\\n토큰화:\\n',result)"]},{"cell_type":"markdown","source":["- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n","- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n","- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n","- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n","- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n","- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"],"metadata":{"id":"_fvkqdg1ATiu"}},{"cell_type":"code","source":["docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n","        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n","\n","token = Tokenizer() #토큰화 함수 지정\n","token.fit_on_texts(docs) #토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력\n","# Tokenizer()의 word_counts함수는 순서를 기억하는 OrderedDict 클래스를 사용\n","print('\\n단어 카운트:\\n', token.word_counts)\n","print('\\n문장 카운트:',token.document_count)\n","print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n","# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n","# 동일한 빈도의 경우는 먼저 등장한 단어가 더 낮은 인덱스를 할당\n","print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkKPSStYAUCn","executionInfo":{"status":"ok","timestamp":1723087761638,"user_tz":-540,"elapsed":372,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"6568f930-0c3a-4cd4-c4ef-e5b61d18d935"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나눈어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트: 3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'단어를': 1, '각': 1, '토큰화합니다': 1, '먼저': 1, '나눈어': 1, '텍스트의': 2, '단어로': 1, '인식됩니다': 1, '토큰화해야': 1, '딥러닝에서': 2, '토큰화한': 1, '있습니다': 1, '결과는': 1, '사용할': 1, '수': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나눈어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"]}]},{"cell_type":"code","source":["docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n","'검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","\n","print('\\n단어 카운트:\\n', token.word_counts)\n","print('\\n문장 카운트:',token.document_count)\n","print('\\n각 단어가 몇개의 문장에 포함되어있는가:\\n', token.word_docs)\n","print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnEPEnBmC_11","executionInfo":{"status":"ok","timestamp":1723088088096,"user_tz":-540,"elapsed":425,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"d369ebd0-19e3-4380-a93a-34f4c3e0e16c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n","\n","문장 카운트: 2\n","\n","각 단어가 몇개의 문장에 포함되어있는가:\n"," defaultdict(<class 'int'>, {'만에': 1, '재판부가': 1, '전부를': 1, '판단하면서': 1, '시름을': 1, '혐의': 1, '3년': 1, '됐다': 1, '기소': 1, '덜게': 1, '5개월여': 1, '사실': 1, '이': 1, '무죄로': 1, '검찰이': 1, '회장은': 1, '제시한': 1, '검찰': 2, '이후': 1, '2심': 1, '진행될': 1, '전망이': 1, '재판이': 1, '항소로': 1, '것이란': 1, '나오지만': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jjd8GcMAE8Hb","executionInfo":{"status":"ok","timestamp":1723088491116,"user_tz":-540,"elapsed":1462,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"f718bd53-ffed-45fb-d3c4-bcb1b51a6e5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQNQXPznFHXZ","executionInfo":{"status":"ok","timestamp":1723088535589,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"5a8948aa-f39a-46e0-b928-72e133eed923"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","# 어간 추출기와 표제어 추출기 초기화\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# 예제 문장\n","sentence = \"과일을 과일이 과일이라고\"\n","tokens = word_tokenize(sentence)\n","\n","# 어간 추출\n","stems = [stemmer.stem(token) for token in tokens]\n","print(\"어간 추출:\", stems)\n","\n","# 표제어 추출\n","lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n","print(\"표제어 추출:\", lemmas)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNCOxAPbEvqh","executionInfo":{"status":"ok","timestamp":1723088539152,"user_tz":-540,"elapsed":1932,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"edf0a94c-8ddb-4061-adac-c01447a7908f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["어간 추출: ['과일을', '과일이', '과일이라고']\n","표제어 추출: ['과일을', '과일이', '과일이라고']\n"]}]},{"cell_type":"code","source":["!pip install konlpy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vd0rUH5QGyAj","executionInfo":{"status":"ok","timestamp":1723088985960,"user_tz":-540,"elapsed":5725,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"9efb52d8-3f64-4cc8-be08-cbc63e81a18b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","\n","# Okt 형태소 분석기 초기화\n","okt = Okt()\n","\n","# 예제 문장\n","sentence = \"과일을 과일이 과일이라고\"\n","\n","# 형태소 분석 및 표제어 추출\n","tokens = okt.morphs(sentence)\n","print(\"형태소 분석:\", tokens)\n","\n","# 표제어 추출\n","# 한국어에서 표제어 추출을 위해서는 형태소 분석을 통해 얻은 결과를 활용\n","# '과일'이 동일한 형태로 유지되지만, 일반적으로 '형태소'가 추출됨.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlp88VTLGyad","executionInfo":{"status":"ok","timestamp":1723089006304,"user_tz":-540,"elapsed":16613,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"6004e236-bc0d-44d3-fbd6-df3454361349"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["형태소 분석: ['과일', '을', '과일', '이', '과일', '이라고']\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","from collections import Counter\n","\n","# Okt 형태소 분석기 초기화\n","okt = Okt()\n","\n","# 예제 문장\n","sentence = \"과일을 과일이 과일이라고\"\n","\n","# 형태소 분석\n","tokens = okt.morphs(sentence)\n","print(\"형태소 분석 결과:\", tokens)\n","\n","# 형태소의 빈도 계산\n","token_counts = Counter(tokens)\n","print(\"형태소 빈도:\", token_counts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NsTcYZSHEso","executionInfo":{"status":"ok","timestamp":1723089049288,"user_tz":-540,"elapsed":558,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"fd659c35-cf38-43b5-c2ee-2c16e7586439"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["형태소 분석 결과: ['과일', '을', '과일', '이', '과일', '이라고']\n","형태소 빈도: Counter({'과일': 3, '을': 1, '이': 1, '이라고': 1})\n"]}]},{"cell_type":"markdown","source":["Keras의 Tokenizer를 사용하여 텍스트 데이터를 정수 인덱스 시퀀스로 변환한 후, 이를 One-Hot Encoding 형식으로 변환하는 과정은 NLP 모델의 입력 데이터를 준비하는 중요한 단계입니다.\n","\n","Tokenizer를 사용한 텍스트 토큰화\n","\n","word_index:\n","- token.word_index는 각 단어를 고유한 정수 인덱스로 매핑한 딕셔너리입니다. 키는 단어이고, 값은 해당 단어의 인덱스입니다.\n","- 이 딕셔너리의 길이(len(token.word_index))는 말뭉치에 있는 고유 단어의 총 개수를 나타냅니다.\n","\n","word_size:\n","- word_size는 고유 단어의 총 개수에 1을 더한 값입니다. 이는 NLP에서 일반적인 관행으로, \"0\" 인덱스를 포함하기 위해 사용됩니다.\n","- \"0\" 인덱스는 패딩(padding)에 사용되거나, 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n","\n","One-Hot Encoding:\n","- to_categorical 함수는 클래스 벡터(정수 인덱스)를 바이너리 클래스 행렬로 변환합니다.\n","- x는 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하며, - to_categorical은 이를 One-Hot Encoding 형식으로 변환합니다.\n","- 각 정수에 대해 해당 인덱스 위치만 1로 설정되고 나머지 위치는 0인 벡터를 생성합니다.\n","\n","num_classes:\n","- num_classes는 총 클래스 수를 지정합니다. 이 경우 어휘 크기(word_size)로 설정되어, One-Hot Encoding에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인합니다."],"metadata":{"id":"v0ore-VAF42K"}},{"cell_type":"markdown","source":["Embedding 레이어\n","- 자연어 처리나 시퀀스 데이터를 다루는 모델에서 자주 사용되는 레이어로, 입력 데이터를 고정된 크기의 고차원 벡터로 변환해주는 역할을 합니다.\n","\n","주요 기능:\n","  - 단어를 벡터로 변환: Embedding 레이어는 고유한 정수 인덱스로 표현된 단어들을 특정 크기의 벡터로 변환합니다. 이 벡터들은 학습 과정에서 모델이 조정할 수 있는 가중치로 초기화됩니다.\n","  - 차원 축소 및 특성 학습: 높은 차원의 단어를 저차원의 벡터로 변환함으로써 데이터의 차원을 축소하고, 단어 간의 유사성을 학습할 수 있습니다. 예를 들어, 비슷한 의미의 단어들이 유사한 벡터로 매핑되도록 학습됩니다.\n","\n","입력 및 출력:\n","  - 입력: 정수로 인코딩된 시퀀스 데이터. 예를 들어, 단어를 고유한 정수로 인코딩한 시퀀스.\n","  - 출력: 각 입력 정수에 대응하는 고정된 크기의 밀집 벡터."],"metadata":{"id":"1RX_d37Fd8Ps"}},{"cell_type":"code","source":["# keras Tokenizer의 fit_on_texts 메소드는 일반적으로 문자열의 리스트를 기대하기 때문에, 단일 문자열을 입력하는 것은 적절하지 않다\n","text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ou1eeJSFM4K","executionInfo":{"status":"ok","timestamp":1723088783557,"user_tz":-540,"elapsed":385,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"9e39feb3-4dc0-48aa-ceab-7f1b1039aaeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}]},{"cell_type":"code","source":["text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","token = Tokenizer()\n","token.fit_on_texts(text)\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gz5wztp6F9_E","executionInfo":{"status":"ok","timestamp":1723088770777,"user_tz":-540,"elapsed":359,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"5ecae937-1f95-4284-9a50-e04de15ef8e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'꿈': 1, '는': 2, '오': 3, '랫': 4, '동': 5, '안': 6, '꾸': 7, '이': 8, '그': 9, '을': 10, '닮': 11, '아': 12, '간': 13, '다': 14}\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjNkzoY8FeWh","executionInfo":{"status":"ok","timestamp":1723088640184,"user_tz":-540,"elapsed":368,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"803f5b4f-68b8-41b6-a114-0643a9ad6b69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}]},{"cell_type":"code","source":["# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n","# word_size에 1을 추가하는 이유는 Keras의 Tokenizer를 사용할 때 0 인덱스를 패딩을 위해 예약하는 관례 때문\n","word_size = len(token.word_index) +1\n","x = to_categorical(x, num_classes=word_size)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jW9orIIRFiDc","executionInfo":{"status":"ok","timestamp":1723088706261,"user_tz":-540,"elapsed":370,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"8ddd4ea9-d171-44c0-cbb4-5896c9ec1834"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Input\n","\n","# 입력 시퀀스의 길이를 3이라고 가정\n","input_data = tf.constant([1,2,3], dtype=tf.int32)\n","# 임베딩 레이어 정의\n","embedding_layer = Embedding(input_dim=100, output_dim=10)\n","# 텐서로 전달해야함\n","output = embedding_layer(input_data)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BTwKDxmfMgl","executionInfo":{"status":"ok","timestamp":1723162744610,"user_tz":-540,"elapsed":345,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"cab516fe-a52b-4e93-d708-85e9ab9fa6d6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[-3.7558388e-02  3.2512639e-02  2.5352802e-02  2.1997143e-02\n","   4.2947087e-02 -4.8430171e-02 -1.9364893e-02 -2.3285603e-02\n","  -2.5157213e-02  3.2960925e-02]\n"," [-2.8330907e-03 -1.0687493e-02  3.7445772e-02  1.5826408e-02\n","   2.1206427e-02  2.3162965e-02  7.6342113e-03 -2.8181219e-02\n","  -2.1670057e-02 -3.9850365e-02]\n"," [ 9.0622194e-03  4.3486405e-02 -3.7891507e-02 -4.7203567e-02\n","  -8.9071691e-05 -1.7407648e-03 -7.1063414e-03  4.1811708e-02\n","  -1.5952073e-02 -1.5902296e-03]], shape=(3, 10), dtype=float32)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n"],"metadata":{"id":"5p2YCsTafnGf","executionInfo":{"status":"ok","timestamp":1723162750722,"user_tz":-540,"elapsed":338,"user":{"displayName":"윤경","userId":"08475145638068530881"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["docs = ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다','한번 더 보고싶네요','글쎄요','별로예요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n","\n","classes = array([1,1,1,1,1,0,0,0,0,0])\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HY9QvRIQhF1N","executionInfo":{"status":"ok","timestamp":1723163146270,"user_tz":-540,"elapsed":363,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"cf8db9a5-081d-4f26-de73-9e1296cb3f98"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences(docs)\n","print('\\n리뷰 텍스트, 토큰화 결과:\\n', x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8erGUflgiB2S","executionInfo":{"status":"ok","timestamp":1723163249501,"user_tz":-540,"elapsed":4,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"3864e91c-9695-42ae-cd64-d5e97ff27756"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","리뷰 텍스트, 토큰화 결과:\n"," [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"]}]},{"cell_type":"code","source":["# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다\n","# padding = 'pre' 시퀀스의 길이가 4보다 짧은 경우 앞쪽을 0으로 채워 길이를 4로 맞춘다\n","padded_x = pad_sequences(x,4,padding='pre')\n","# pre앞에 채워짐 post 뒤에 채워짐\n","\n","print('\\n패딩 결과:\\n',padded_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M1n_-yG6jYeG","executionInfo":{"status":"ok","timestamp":1723163721861,"user_tz":-540,"elapsed":368,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"0a99c58a-ed74-48cf-896c-486584041fad"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","패딩 결과:\n"," [[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [ 0 11 12 13]\n"," [ 0  0  0 14]\n"," [ 0  0  0 15]\n"," [ 0  0 16 17]\n"," [ 0  0 18 19]\n"," [ 0  0  0 20]]\n"]}]},{"cell_type":"markdown","source":["### Embedding\n","- 기계 학습, 특히 자연어 처리(NLP)의 맥락에서 임베딩은 단어, 구문 또는 기타 유형의 엔터티를 실수의 밀집된 벡터로 표현하는 기술을 의미. 핵심 아이디어는 이러한 엔터티의 의미론적 의미를 연속적인 벡터 공간으로 인코딩하는 것이다. 여기서 벡터 간의 기하학적 관계는 이들이 나타내는 엔터티 간의 의미론적 관계를 반영하며 이 접근 방식은 의미론적 관계가 캡처되지 않는 원-핫 인코딩과 같은 희소 표현과 대조된다.\n","- 의미:\n","  - 의미적 표현: 임베딩 벡터는 단어나 개체의 의미를 포착하도록 설계된다. 비슷한 의미를 가진 단어는 임베딩 공간에서 서로 가까운 벡터를 갖도록 처리된다.\n","  - 차원성 감소: 임베딩은 고차원 공간(예: 원-핫 인코딩된 벡터)의 단어를 저차원의 연속 벡터 공간으로 매핑하며 이는 표현을 더욱 효율적으로 만들고 계산 복잡성을 줄인다.\n","  - 컨텍스트화: 고급 임베딩 모델(예: Word2Vec, GloVe, BERT)에서는 단어가 나타나는 컨텍스트가 벡터 표현에 영향을 미치므로 모델이 다양한 컨텍스트에서 단어의 다양한 의미를 캡처할 수 있다.\n","- 임베딩 생성 방법:\n","  - 사전 훈련된 임베딩: 일반적인 접근 방식 중 하나는 대규모 텍스트 모음에 대해 사전 훈련된 임베딩을 사용하는 것으로 Word2Vec, GloVe와 같은 모델을 사용하면 훈련 코퍼스에서 풍부한 의미 체계 관계를 학습한 사전 훈련된 모델을 기반으로 단어를 벡터에 매핑할 수 있다.\n","    - Word2Vec: 컨텍스트를 사용하여 대상 단어를 예측하거나(CBOW) 단어를 사용하여 컨텍스트를 예측(Skip-gram)하여 임베딩을 학습한다.\n","      - CBOW 모델은 주어진 컨텍스트(주변 단어들)를 바탕으로 타겟 단어를 예측하는 방식으로 작동. 예를 들어, 문장 \"the cat sits on the\"에서 \"cat\", \"sits\", \"on\", \"the\"를 컨텍스트로 사용하여 \"mat\"이라는 타겟 단어를 예측\n","      - skip-gram 모델은 CBOW와 반대로, 주어진 타겟 단어로부터 컨텍스트(주변 단어들)를 예측하는 방식으로 작동. 예를 들어, \"cat\"이라는 단어가 주어졌을 때, \"the\", \"sits\", \"on\", \"the\"와 같은 주변 단어들을 예측\n","    - GloVe: 단어 동시 발생 통계 행렬을 인수분해하여 임베딩을 학습한다.    \n","  - 자신만의 임베딩 훈련: 신경망을 사용하여 처음부터 자신만의 임베딩을 훈련할 수도 있다."],"metadata":{"id":"3cXMtam3lJLe"}},{"cell_type":"markdown","source":["텍스트 입력을 표현하기 위해 단어 임베딩 활용\n","- model.add(Embedding(word_size, 8, input_length=4)): 이 줄은 모델에 임베딩 레이어를 추가. 임베딩 레이어는 단어 임베딩을 만드는 데 사용. 각 매개변수의 의미는 다음과 같다.\n","  - word_size: 입력 차원의 크기, 즉 어휘 크기. 데이터 세트에 있는 총 고유 단어 수에 1을 더한 값.\n","  - 8: 임베딩 벡터의 크기. 각 단어는 8차원 벡터로 표현.\n","  - input_length=4: 입력 시퀀스의 길이. 이 모델은 각 입력 시퀀스의 길이가 4일 것으로 예상(예: 입력당 4개의 단어).\n","- model.add(Flatten()): 임베딩 레이어 이후 출력 모양은 배치 크기를 포함하여 3차원. 이 레이어는 출력을 2차원(배치 크기, input_length * 8)으로 평면화하여 밀도가 높은 레이어에 직접 공급될 수 있도록 한다."],"metadata":{"id":"axwcUPtSmdlP"}},{"cell_type":"code","source":["import numpy as np\n","\n","docs = ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다','한번 더 보고싶네요','글쎄요','별로예요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n","\n","classes = np.array([1,1,1,1,1,0,0,0,0,0])\n"],"metadata":{"id":"x67p5IY8j84s","executionInfo":{"status":"ok","timestamp":1723164512350,"user_tz":-540,"elapsed":410,"user":{"displayName":"윤경","userId":"08475145638068530881"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","\n","print(token.word_index)\n","\n","word_size = len(token.word_index) +1\n","\n","model = Sequential()\n","model.add(Input(shape=(4,)))\n","model.add(Embedding(word_size, 8))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":260},"id":"4ePj3qFBmzT0","executionInfo":{"status":"ok","timestamp":1723164716618,"user_tz":-540,"elapsed":352,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"09b13aa2-f297-453c-880f-65a4167882f6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)                │             \u001b[38;5;34m168\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">168</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201\u001b[0m (804.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> (804.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m201\u001b[0m (804.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> (804.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["x = token.texts_to_sequences(docs)\n","padded_x = pad_sequences(x, 4, padding='pre')"],"metadata":{"id":"jiQI-ZiPqvhO","executionInfo":{"status":"ok","timestamp":1723165938877,"user_tz":-540,"elapsed":366,"user":{"displayName":"윤경","userId":"08475145638068530881"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model.fit(padded_x, classes, epochs=20)\n","print('\\n Accuracy: %.4f' % (model.evaluate(padded_x, classes)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eBXBjFvsZpo","executionInfo":{"status":"ok","timestamp":1723166026566,"user_tz":-540,"elapsed":2223,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"9355aa73-05ff-442f-d829-3006c99345fe"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856ms/step - accuracy: 0.9000 - loss: 0.6517\n","Epoch 2/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9000 - loss: 0.6497\n","Epoch 3/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9000 - loss: 0.6478\n","Epoch 4/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9000 - loss: 0.6458\n","Epoch 5/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9000 - loss: 0.6438\n","Epoch 6/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9000 - loss: 0.6418\n","Epoch 7/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9000 - loss: 0.6397\n","Epoch 8/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9000 - loss: 0.6377\n","Epoch 9/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9000 - loss: 0.6357\n","Epoch 10/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9000 - loss: 0.6336\n","Epoch 11/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9000 - loss: 0.6316\n","Epoch 12/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9000 - loss: 0.6295\n","Epoch 13/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9000 - loss: 0.6274\n","Epoch 14/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9000 - loss: 0.6253\n","Epoch 15/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9000 - loss: 0.6232\n","Epoch 16/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9000 - loss: 0.6211\n","Epoch 17/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9000 - loss: 0.6190\n","Epoch 18/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9000 - loss: 0.6168\n","Epoch 19/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9000 - loss: 0.6147\n","Epoch 20/20\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9000 - loss: 0.6125\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.9000 - loss: 0.6103\n","\n"," Accuracy: 0.9000\n"]}]},{"cell_type":"markdown","source":["Q. 10개의 단어를 가진 어휘(vocabulary)와 각 단어를 4차원 벡터로 임베딩하는 Keras 모델을 구성하세요. 시퀀스의 최대 길이는 5로 설정하세요.\n","\n","`input_data = np.array([\n","    [1, 2, 3, 4, 5],   \n","    [2, 3, 4, 5, 6]    \n","])`"],"metadata":{"id":"43gRVytYtRYr"}},{"cell_type":"code","source":["input_data = np.array([ [1, 2, 3, 4, 5], [2, 3, 4, 5, 6] ])\n","\n","model = Sequential()\n","\n","model.add(Input(shape=(5,)))\n","model.add(Embedding(input_dim=10, output_dim=4))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam',loss='binary_crossentropy')\n","\n","model.summary()\n","\n","output = model.predict(input_data)\n","print(\"임베딩 레이어의 출력:\\n\", output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"OwnF18bFu3nj","executionInfo":{"status":"ok","timestamp":1723166783901,"user_tz":-540,"elapsed":421,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"ec03a837-a5c5-4305-97e9-e26c23260d59"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_2\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m4\u001b[0m)                │              \u001b[38;5;34m40\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61\u001b[0m (244.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> (244.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61\u001b[0m (244.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> (244.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n","임베딩 레이어의 출력:\n"," [[0.5062941]\n"," [0.5062498]]\n"]}]},{"cell_type":"markdown","source":["Q. imdb 영화 리뷰 데이터셋을 사용하여 긍부정 이진분류 모델링 및 평가를 수행"],"metadata":{"id":"PUzZ8IJD01-L"}},{"cell_type":"code","source":["X_train.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yei7v3fU244y","executionInfo":{"status":"ok","timestamp":1723168694576,"user_tz":-540,"elapsed":363,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"806cc630-0dc7-4914-c765-0070bc6520c7"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9999"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from keras.datasets import imdb\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","# num_words 훈련 데이터에서 가장 자주 나타나는 상위 만개의 단어만 사용하겠다는 의미\n","(X_train, y_train),(X_test, y_test) = imdb.load_data(num_words=10000)\n","\n","X_train = pad_sequences(X_train, maxlen=100)\n","X_test = pad_sequences(X_test, maxlen=100)\n","\n","model = Sequential()\n","model.add(Input(shape=(100,))) # 모든 입력 데이터를 동일한 길이로 맞춰야 하기 때문에 패딩 사용 후 100으로 맞춤\n","model.add(Embedding(10000,8)) # 입력 차원은 단어 인덱스의 최대값 +1, 출력 차원은 임베딩 후 벡터 크기\n","# 임베딩 레이어는 모델이 학습할 수 있는 단어 인덱스의 총 개수를 설정\n","model.add(Flatten()) # 임베딩된 시퀀스를 평탄화\n","model.add(Dense(1, activation='sigmoid')) # 긍정과 부정을 분류하므로 시그모이드 활성화 함수 사용\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n","\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Accracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":659},"id":"RM9ZTBpiu9jL","executionInfo":{"status":"ok","timestamp":1723167712425,"user_tz":-540,"elapsed":33933,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"7a46c2c1-2b20-4044-bf90-118ed800a4c3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_3\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │          \u001b[38;5;34m80,000\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m801\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,801\u001b[0m (315.63 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,801</span> (315.63 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,801\u001b[0m (315.63 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,801</span> (315.63 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5930 - loss: 0.6644 - val_accuracy: 0.8278 - val_loss: 0.4132\n","Epoch 2/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8656 - loss: 0.3440 - val_accuracy: 0.8412 - val_loss: 0.3464\n","Epoch 3/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9127 - loss: 0.2401 - val_accuracy: 0.8538 - val_loss: 0.3328\n","Epoch 4/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9439 - loss: 0.1768 - val_accuracy: 0.8462 - val_loss: 0.3437\n","Epoch 5/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9634 - loss: 0.1305 - val_accuracy: 0.8520 - val_loss: 0.3575\n","Epoch 6/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9813 - loss: 0.0913 - val_accuracy: 0.8456 - val_loss: 0.3810\n","Epoch 7/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9917 - loss: 0.0614 - val_accuracy: 0.8428 - val_loss: 0.4004\n","Epoch 8/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0433 - val_accuracy: 0.8420 - val_loss: 0.4305\n","Epoch 9/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9976 - loss: 0.0303 - val_accuracy: 0.8404 - val_loss: 0.4535\n","Epoch 10/10\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0185 - val_accuracy: 0.8416 - val_loss: 0.4817\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8360 - loss: 0.4887\n","Test Accracy: 0.8359599709510803\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"I0mzU-tI-OSG"}},{"cell_type":"markdown","source":["Output Shape는 해당 레이어의 출력 텐서의 형태(Shape)를 나타냅니다. 모델에서 각 레이어가 입력을 받아서 어떤 형태로 출력을 생성하는지를 보여줍니다.\n","\n","각 레이어의 Output Shape을 해석해보면:\n","\n","Embedding Layer (embedding_3):\n","\n","Output Shape: (None, 100, 8)\n","None: 배치 크기를 의미합니다. 훈련 중에는 다양한 크기의 배치를 사용할 수 있기 때문에 None으로 표시됩니다.\n","100: 입력된 시퀀스(리뷰)의 길이를 나타냅니다. 이 경우 길이 100의 단어 인덱스를 입력받습니다.\n","8: 각 단어 인덱스가 변환된 임베딩 벡터의 차원입니다. 즉, 각 단어가 8차원 벡터로 표현됩니다.\n","Flatten Layer (flatten_2):\n","\n","Output Shape: (None, 800)\n","800은 100개의 단어가 각각 8차원 벡터로 변환되었기 때문에 계산된 값입니다:\n","100\n","×\n","8\n","=\n","800\n","100×8=800.\n","Flatten 레이어는 다차원 배열을 1차원 배열로 변환합니다.\n","Dense Layer (dense_2):\n","\n","Output Shape: (None, 1)\n","1: 출력이 1개의 노드로, 이진 분류 문제이므로 긍정(1) 또는 부정(0) 중 하나의 값을 예측합니다.\n","2. Param\n","**Param #**는 해당 레이어에서 학습해야 하는 파라미터의 수를 나타냅니다. 파라미터는 모델이 학습하는 가중치와 바이어스 등을 포함합니다.\n","\n","각 레이어의 파라미터 수를 해석해보면:\n","\n","Embedding Layer (embedding_3):\n","\n","Param #: 80,000\n","임베딩 레이어의 파라미터 수는\n","10\n",",\n","000\n"," (단어 수)\n","×\n","8\n"," (임베딩 차원)\n","=\n","80\n",",\n","000\n","10,000 (단어 수)×8 (임베딩 차원)=80,000입니다. 각 단어 인덱스에 대해 8차원의 가중치 벡터가 존재하므로 총 80,000개의 파라미터가 있습니다.\n","Flatten Layer (flatten_2):\n","\n","Param #: 0\n","Flatten 레이어는 단순히 텐서의 차원을 변경하는 역할을 하므로, 학습할 파라미터가 없습니다.\n","Dense Layer (dense_2):\n","\n","Param #: 801\n","이 레이어의 파라미터 수는 가중치와 바이어스를 포함합니다.\n","가중치 수는\n","800\n"," (입력 노드 수)\n","×\n","1\n"," (출력 노드 수)\n","=\n","800\n","800 (입력 노드 수)×1 (출력 노드 수)=800이며, 여기에 바이어스 1개가 추가되어 총 801개의 파라미터가 됩니다."],"metadata":{"id":"SnvqFYhn-JPt"}},{"cell_type":"markdown","source":["Output Shape: 각 레이어의 출력 형태를 나타냅니다.\n","Param #: 해당 레이어에서 학습해야 하는 파라미터의 수를 나타냅니다."],"metadata":{"id":"gpcPzGNr-Ija"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# 예측할 새로운 텍스트 데이터\n","new_reviews = [\n","    \"This movie was fantastic! I really enjoyed it.\",\n","    \"Terrible movie, I will never watch it again.\",\n","    \"It was an average film, nothing special.\",\n","    \"Absolutely loved the movie, great acting and story!\",\n","    \"Worst movie ever, completely wasted my time.\"\n","]\n","\n","# 이미 학습된 토크나이저로 시퀀스를 변환\n","word_index = imdb.get_word_index() # 함수는 IMDB 데이터셋에 사용된 단어와 그 단어에 대응하는 인덱스를 매핑한 딕셔너리를 반환\n","tokenizer = Tokenizer(num_words=10000)\n","# v + 3은 원래 IMDB 단어 인덱스에서 사용된 정수 인덱스 값에 3을 더한 새로운 인덱스를 생성\n","tokenizer.word_index = {k: (v + 3) for k, v in word_index.items()} # 특정 예약된 인덱스(예: <PAD>, <START>, <UNK> 등)를 위해 앞부분의 인덱스를 비워두기 위함\n","tokenizer.word_index[\"<PAD>\"] = 0 # 패딩 토큰을 나타내며, 인덱스 0을 부여\n","tokenizer.word_index[\"<START>\"] = 1 # 시퀀스의 시작을 나타내는 토큰이며, 인덱스 1을 부여\n","tokenizer.word_index[\"<UNK>\"] = 2 # 인덱스에 포함되지 않은 단어들을 대체하기 위해 사용됩니다. 인덱스 2를 부여\n","tokenizer.word_index[\"<UNUSED>\"] = 3 # 사용되지 않는 토큰을 위해 인덱스 3을 부여\n","\n","sequences = tokenizer.texts_to_sequences(new_reviews)\n","padded_sequences = pad_sequences(sequences, maxlen=100)\n","\n","# 예측 수행\n","predictions = model.predict(padded_sequences)\n","\n","# 예측 결과 출력\n","for i, review in enumerate(new_reviews):\n","    print(f\"Review: {review}\")\n","    print(f\"Predicted Sentiment: {'Positive' if predictions[i] > 0.5 else 'Negative'}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBZXY93M8nhg","executionInfo":{"status":"ok","timestamp":1723170230217,"user_tz":-540,"elapsed":589,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"67a3564c-8a8c-4b4d-c807-f519b17a34e7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","Review: This movie was fantastic! I really enjoyed it.\n","Predicted Sentiment: Negative\n","\n","Review: Terrible movie, I will never watch it again.\n","Predicted Sentiment: Negative\n","\n","Review: It was an average film, nothing special.\n","Predicted Sentiment: Negative\n","\n","Review: Absolutely loved the movie, great acting and story!\n","Predicted Sentiment: Negative\n","\n","Review: Worst movie ever, completely wasted my time.\n","Predicted Sentiment: Negative\n","\n"]}]}]}