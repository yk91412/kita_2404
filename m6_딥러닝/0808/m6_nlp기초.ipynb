{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP16S8tPR1LmiW7H44QAL03"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 텍스트의 토큰화\n","**자연어 처리(NLP)에서 토큰화(Tokenization)**\n","- 텍스트 분석, 언어 번역, 감정 분석 등 다양한 NLP 작업을 위해 텍스트를 준비하는 기본 단계입니다.\n","- 토큰화는 텍스트를 단어, 문구, 기호 또는 기타 의미 있는 요소(토큰)로 분해하는 작업을 포함합니다. 이 과정에서 생성된 토큰은 추가 처리 및 분석을 위한 기본 구성 요소가 됩니다.\n","\n","토큰화의 목적\n","- 토큰화의 주요 목적은 텍스트 데이터를 단순화하여 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것입니다. 이를 통해 텍스트의 복잡성을 줄이고 일관성을 유지함으로써, 다양한 NLP 작업에서 효율적인 분석과 처리가 가능해집니다.\n","\n","토큰화의 유형\n","- 토큰화는 다양한 수준에서 수행될 수 있으며, 각 유형은 특정 NLP 작업의 요구 사항에 따라 선택됩니다:\n","  - 단어 토큰화 (Word Tokenization):\n","    텍스트를 개별 단어로 분해합니다.\n","    예: \"ChatGPT is amazing!\" → [\"ChatGPT\", \"is\", \"amazing\", \"!\"]\n","  - 문장 토큰화 (Sentence Tokenization):\n","    텍스트를 개별 문장으로 분해합니다.\n","    예: \"Hello world. How are you?\" → [\"Hello world.\", \"How are you?\"]\n","  - 하위 단어 토큰화 (Subword Tokenization):\n","    단어를 더 작은 의미 단위로 분해합니다. 주로 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 사용합니다.\n","    예: \"unhappiness\" → [\"un\", \"hap\", \"pi\", \"ness\"]\n","  - 문자 토큰화 (Character Tokenization):\n","    텍스트를 개별 문자로 분해합니다.\n","    예: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n","\n","토큰화의 과정\n","- 토큰화 과정은 일반적으로 다음 단계로 구성됩니다:\n","  - 텍스트 정규화 (Text Normalization):\n","    모든 텍스트를 소문자로 변환하여 일관성을 유지합니다.\n","    불필요한 구두점과 공백을 제거합니다.\n","    예: \"Hello, World!\" → \"hello world\"\n","  - 구분자 사용 (Delimiter-based Tokenization):\n","    공백이나 구두점을 기준으로 텍스트를 분해합니다.\n","    예: \"hello world\" → [\"hello\", \"world\"]\n","  - 고급 토큰화 기법 (Advanced Tokenization Techniques):\n","    언어의 문법적, 의미적 구조를 고려하여 토큰을 생성합니다.\n","    BPE, WordPiece, SentencePiece 등의 알고리즘을 사용합니다.\n","\n","토큰화의 중요성\n","- 토큰화는 NLP 작업에서 매우 중요한 역할을 합니다. 잘못된 토큰화는 후속 처리와 분석의 정확도에 큰 영향을 미칠 수 있습니다.\n","- 반면, 올바른 토큰화는 텍스트 데이터를 효과적으로 전처리하고 분석할 수 있게 합니다."],"metadata":{"id":"k6vXLxBB8Z3h"}},{"cell_type":"markdown","source":["## 고급 토큰화 기법\n","언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.\n","\n","\n","형태소 분석 (Morphological Analysis):\n","- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.\n","- 예를 들어, \"cats\"는 \"cat\"과 복수형 접미사 \"s\"로 분해됩니다.\n","- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.\n","\n","어간 추출 (Stemming):\n","- 단어의 어간을 추출하여 형태를 단순화합니다.\n","- 예: \"running\", \"runs\", \"ran\" → \"run\"\n","- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.\n","\n","어근 추출 (Lemmatization):\n","- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.\n","- 예: \"running\", \"ran\" → \"run\"\n","- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, \"better\"는 어근 \"good\"으로 변환됩니다.\n","- WordNetLemmatizer와 같은 도구가 사용됩니다.\n","\n","BPE (Byte Pair Encoding):\n","- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.\n","- 예: \"lowest\"가 \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"로 분해되고, 자주 등장하는 \"lo\", \"we\"가 결합되어 \"low\", \"est\"로 변환됩니다.\n","- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.\n","\n","WordPiece:\n","- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.\n","- 예: \"unhappiness\" → [\"un\", \"##happiness\"]\n","- 트랜스포머 모델(BERT 등)에서 사용됩니다.\n","\n","SentencePiece:\n","- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.\n","- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.\n","- 예: \"unhappiness\" → [\"un\", \"ha\", \"ppiness\"]\n","- Google의 T5, ALBERT 모델에서 사용됩니다."],"metadata":{"id":"e0_EHWPV-CcD"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpOf9M-Y8T34","executionInfo":{"status":"ok","timestamp":1723087067056,"user_tz":-540,"elapsed":377,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"7630294c-b5e9-4572-bff2-cf407a00ff6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","원문: \n"," 해보지 않으면 해낼 수 없다\n","\n","토큰화:\n"," ['해보지', '않으면', '해낼', '수', '없다']\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n","# 텍스트 전처리와 관련한 함수 중  text_to_word_sequence를 부러오기\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","text = '해보지 않으면 해낼 수 없다'\n","\n","result = text_to_word_sequence(text)\n","print('\\n원문: \\n',text)\n","print('\\n토큰화:\\n',result)"]},{"cell_type":"markdown","source":["- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n","- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n","- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n","- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n","- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n","- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"],"metadata":{"id":"_fvkqdg1ATiu"}},{"cell_type":"code","source":["docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n","        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n","\n","token = Tokenizer() #토큰화 함수 지정\n","token.fit_on_texts(docs) #토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력\n","# Tokenizer()의 word_counts함수는 순서를 기억하는 OrderedDict 클래스를 사용\n","print('\\n단어 카운트:\\n', token.word_counts)\n","print('\\n문장 카운트:',token.document_count)\n","print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n","# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n","# 동일한 빈도의 경우는 먼저 등장한 단어가 더 낮은 인덱스를 할당\n","print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkKPSStYAUCn","executionInfo":{"status":"ok","timestamp":1723087761638,"user_tz":-540,"elapsed":372,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"6568f930-0c3a-4cd4-c4ef-e5b61d18d935"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나눈어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트: 3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'단어를': 1, '각': 1, '토큰화합니다': 1, '먼저': 1, '나눈어': 1, '텍스트의': 2, '단어로': 1, '인식됩니다': 1, '토큰화해야': 1, '딥러닝에서': 2, '토큰화한': 1, '있습니다': 1, '결과는': 1, '사용할': 1, '수': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나눈어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"]}]},{"cell_type":"code","source":["docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n","'검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","\n","print('\\n단어 카운트:\\n', token.word_counts)\n","print('\\n문장 카운트:',token.document_count)\n","print('\\n각 단어가 몇개의 문장에 포함되어있는가:\\n', token.word_docs)\n","print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnEPEnBmC_11","executionInfo":{"status":"ok","timestamp":1723088088096,"user_tz":-540,"elapsed":425,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"d369ebd0-19e3-4380-a93a-34f4c3e0e16c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n","\n","문장 카운트: 2\n","\n","각 단어가 몇개의 문장에 포함되어있는가:\n"," defaultdict(<class 'int'>, {'만에': 1, '재판부가': 1, '전부를': 1, '판단하면서': 1, '시름을': 1, '혐의': 1, '3년': 1, '됐다': 1, '기소': 1, '덜게': 1, '5개월여': 1, '사실': 1, '이': 1, '무죄로': 1, '검찰이': 1, '회장은': 1, '제시한': 1, '검찰': 2, '이후': 1, '2심': 1, '진행될': 1, '전망이': 1, '재판이': 1, '항소로': 1, '것이란': 1, '나오지만': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jjd8GcMAE8Hb","executionInfo":{"status":"ok","timestamp":1723088491116,"user_tz":-540,"elapsed":1462,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"f718bd53-ffed-45fb-d3c4-bcb1b51a6e5f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQNQXPznFHXZ","executionInfo":{"status":"ok","timestamp":1723088535589,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"5a8948aa-f39a-46e0-b928-72e133eed923"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","# 어간 추출기와 표제어 추출기 초기화\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# 예제 문장\n","sentence = \"과일을 과일이 과일이라고\"\n","tokens = word_tokenize(sentence)\n","\n","# 어간 추출\n","stems = [stemmer.stem(token) for token in tokens]\n","print(\"어간 추출:\", stems)\n","\n","# 표제어 추출\n","lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n","print(\"표제어 추출:\", lemmas)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNCOxAPbEvqh","executionInfo":{"status":"ok","timestamp":1723088539152,"user_tz":-540,"elapsed":1932,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"edf0a94c-8ddb-4061-adac-c01447a7908f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["어간 추출: ['과일을', '과일이', '과일이라고']\n","표제어 추출: ['과일을', '과일이', '과일이라고']\n"]}]},{"cell_type":"code","source":["!pip install konlpy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vd0rUH5QGyAj","executionInfo":{"status":"ok","timestamp":1723088985960,"user_tz":-540,"elapsed":5725,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"9efb52d8-3f64-4cc8-be08-cbc63e81a18b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","\n","# Okt 형태소 분석기 초기화\n","okt = Okt()\n","\n","# 예제 문장\n","sentence = \"과일을 과일이 과일이라고\"\n","\n","# 형태소 분석 및 표제어 추출\n","tokens = okt.morphs(sentence)\n","print(\"형태소 분석:\", tokens)\n","\n","# 표제어 추출\n","# 한국어에서 표제어 추출을 위해서는 형태소 분석을 통해 얻은 결과를 활용\n","# '과일'이 동일한 형태로 유지되지만, 일반적으로 '형태소'가 추출됨.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlp88VTLGyad","executionInfo":{"status":"ok","timestamp":1723089006304,"user_tz":-540,"elapsed":16613,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"6004e236-bc0d-44d3-fbd6-df3454361349"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["형태소 분석: ['과일', '을', '과일', '이', '과일', '이라고']\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","from collections import Counter\n","\n","# Okt 형태소 분석기 초기화\n","okt = Okt()\n","\n","# 예제 문장\n","sentence = \"과일을 과일이 과일이라고\"\n","\n","# 형태소 분석\n","tokens = okt.morphs(sentence)\n","print(\"형태소 분석 결과:\", tokens)\n","\n","# 형태소의 빈도 계산\n","token_counts = Counter(tokens)\n","print(\"형태소 빈도:\", token_counts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NsTcYZSHEso","executionInfo":{"status":"ok","timestamp":1723089049288,"user_tz":-540,"elapsed":558,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"fd659c35-cf38-43b5-c2ee-2c16e7586439"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["형태소 분석 결과: ['과일', '을', '과일', '이', '과일', '이라고']\n","형태소 빈도: Counter({'과일': 3, '을': 1, '이': 1, '이라고': 1})\n"]}]},{"cell_type":"markdown","source":["Keras의 Tokenizer를 사용하여 텍스트 데이터를 정수 인덱스 시퀀스로 변환한 후, 이를 One-Hot Encoding 형식으로 변환하는 과정은 NLP 모델의 입력 데이터를 준비하는 중요한 단계입니다.\n","\n","Tokenizer를 사용한 텍스트 토큰화\n","\n","word_index:\n","- token.word_index는 각 단어를 고유한 정수 인덱스로 매핑한 딕셔너리입니다. 키는 단어이고, 값은 해당 단어의 인덱스입니다.\n","- 이 딕셔너리의 길이(len(token.word_index))는 말뭉치에 있는 고유 단어의 총 개수를 나타냅니다.\n","\n","word_size:\n","- word_size는 고유 단어의 총 개수에 1을 더한 값입니다. 이는 NLP에서 일반적인 관행으로, \"0\" 인덱스를 포함하기 위해 사용됩니다.\n","- \"0\" 인덱스는 패딩(padding)에 사용되거나, 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n","\n","One-Hot Encoding:\n","- to_categorical 함수는 클래스 벡터(정수 인덱스)를 바이너리 클래스 행렬로 변환합니다.\n","- x는 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하며, - to_categorical은 이를 One-Hot Encoding 형식으로 변환합니다.\n","- 각 정수에 대해 해당 인덱스 위치만 1로 설정되고 나머지 위치는 0인 벡터를 생성합니다.\n","\n","num_classes:\n","- num_classes는 총 클래스 수를 지정합니다. 이 경우 어휘 크기(word_size)로 설정되어, One-Hot Encoding에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인합니다."],"metadata":{"id":"v0ore-VAF42K"}},{"cell_type":"code","source":["# keras Tokenizer의 fit_on_texts 메소드는 일반적으로 문자열의 리스트를 기대하기 때문에, 단일 문자열을 입력하는 것은 적절하지 않다\n","text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ou1eeJSFM4K","executionInfo":{"status":"ok","timestamp":1723088783557,"user_tz":-540,"elapsed":385,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"9e39feb3-4dc0-48aa-ceab-7f1b1039aaeb"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}]},{"cell_type":"code","source":["text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","token = Tokenizer()\n","token.fit_on_texts(text)\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gz5wztp6F9_E","executionInfo":{"status":"ok","timestamp":1723088770777,"user_tz":-540,"elapsed":359,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"5ecae937-1f95-4284-9a50-e04de15ef8e4"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{'꿈': 1, '는': 2, '오': 3, '랫': 4, '동': 5, '안': 6, '꾸': 7, '이': 8, '그': 9, '을': 10, '닮': 11, '아': 12, '간': 13, '다': 14}\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjNkzoY8FeWh","executionInfo":{"status":"ok","timestamp":1723088640184,"user_tz":-540,"elapsed":368,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"803f5b4f-68b8-41b6-a114-0643a9ad6b69"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}]},{"cell_type":"code","source":["# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n","# word_size에 1을 추가하는 이유는 Keras의 Tokenizer를 사용할 때 0 인덱스를 패딩을 위해 예약하는 관례 때문\n","word_size = len(token.word_index) +1\n","x = to_categorical(x, num_classes=word_size)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jW9orIIRFiDc","executionInfo":{"status":"ok","timestamp":1723088706261,"user_tz":-540,"elapsed":370,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"8ddd4ea9-d171-44c0-cbb4-5896c9ec1834"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}]}]}