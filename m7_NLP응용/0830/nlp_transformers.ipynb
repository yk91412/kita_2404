{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOmkp6PsBmiHNgoQiyc9GJC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Hugging Face의 'transformers' 기반 번역 챗봇(ko to en)\n","- 모델은 OPUS-MT 프로젝트의 일부로 한영 번역 작업을 위해 사전 학습된 'Helsinki-NLP/opus-mt-ko-en' 사용\n","- AutoTokenizer: 이 클래스는 모델에 대한 텍스트 입력 전처리를 담당합니다. 텍스트를 토큰으로 분할하고, 모델에 필요한 특수 토큰을 추가하고, 토큰을 모델 어휘의 해당 ID로 변환하는 등 텍스트를 모델이 이해할 수 있는 형식으로 변환(토큰화)\n","- AutoModelForSeq2SeqLM: 이 클래스는 번역을 포함하는 시퀀스 간 언어 모델링에 적합한 모델 아키텍처를 로드합니다. 이 모델은 번역과 같은 작업에서 변환기가 작동하는 방식을 이해하는 데 기본이 되는 인코더-디코더 구조를 사용\n","-  KoreanToEnglishTranslator 인스턴스가 생성되면 지정된 model_name을 사용하여 토크나이저와 모델을 초기화\n","- 토큰화: 한국어 텍스트를 모델이 처리할 수 있는 형식으로 토큰화합니다. return_tensors=\"pt\"는 출력이 PyTorch 텐서임을 나타내며 padding=True는 모든 시퀀스가 ​​동일한 길이로 채워지도록 보장\n","- 모델 생성: 'generate' 메소드가 모델 객체에서 호출됩니다. 이 방법은 인코더가 먼저 입력 시퀀스(한국어 텍스트)를 일련의 표현으로 인코딩하는 전체 시퀀스 간 생성 프로세스를 캡슐화합니다. 그런 다음 디코더는 이러한 표현을 사용하여 한 번에 하나의 토큰씩 출력 시퀀스(영어 번역)를 생성\n","- 디코딩: decode 메서드는 생성된 토큰 ID를 다시 읽을 수 있는 텍스트로 변환하고 처리에 사용된 특수 토큰을 생략\n","\n","\n"],"metadata":{"id":"tHxM8pGAQIxc"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eb_bnEFPq2K","executionInfo":{"status":"ok","timestamp":1724988042125,"user_tz":-540,"elapsed":7558,"user":{"displayName":"윤경","userId":"08475145638068530881"}},"outputId":"4f786eca-a568-425c-c9a2-f32f77a2dfda"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"stream","name":"stdout","text":["Korean Text: 인공지능 진화 속도가 아주 빠르네요\n","English Text: The rate at which artificial intelligence evolves is very rapid.\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","class KoreanToEnglishTranslator:\n","    def __init__(self, model_name:str):\n","        \"\"\"\n","        Initializes the translator model and tokenizer.\n","        Args:\n","        model_name: The name of the model to load from Hugging Face's model hub.\n","        \"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","    def translate(self,text: str) -> str:\n","        \"\"\"\n","        Translates Korean text to English.\n","        Args:\n","        text: The Korean text to translate.\n","\n","        Returns:\n","        The translated English text.\n","        \"\"\"\n","        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True)\n","\n","        outputs = self.model.generate(**inputs)\n","\n","        translated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return translated_text\n","\n","model_name = \"Helsinki-NLP/opus-mt-ko-en\" # Example model for Korean to English translation\n","translator = KoreanToEnglishTranslator(model_name)\n","\n","korean_text = \"인공지능 진화 속도가 아주 빠르네요\"\n","english_translation = translator.translate(korean_text)\n","print(f\"Korean Text: {korean_text}\")\n","print(f\"English Text: {english_translation}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"SreYwXeuSNyC"},"execution_count":null,"outputs":[]}]}