

## 행렬곱을 할 때 연산 순서

  연산 순서에 따라 결과가 달라질 수 있다

  => 연산의 의미에 따라 결정

    1. 입력 벡터 X를 가중치 행렬 W에 곱하여 새로운 출력 벡터를 계산하는 경우

        np.dot(X,W)사용

    2. 가중치 행렬 W를 입력 벡터 X에 곱하여 각 출력 뉴런에 대한 값을 계산하는 경우

        np.dot(W,X)를 사용


  ** numpy의 행렬 연산에 대한 유연성

      p = np.array([1, 2]), T = np.array([[2, 0], [0, 3]]) 일때

      p_prime = np.dot(T,p) p1 = np.dot(p,T) 둘의 결과값은 같다

      1행 3열, 3행 1열 => 1행 3열로 출력됨

      *** 3행 1열 벡터(2차원)와 3행 3열 행렬을 곱할 땐 3행 1열의 값으로 출력됨

=========================================================================

  ## 고유값, 고유벡터

    주어진 행렬 A와 고유값 λ, 그리고 고유벡터 v 사이의 관계는 다음과 같은 방정식으로 표현

    - Av = λv 

     * A는  n×n  행렬

     * v는 0이 아닌 n차원 벡터로, A의 고유벡터

     * λ는 스칼라 값으로, A의 고유값

      고유값의 개수는 행렬의 크기에 따라 결정

        => 주어진 n×n 정사각 행렬 A는 최대 n개의 고유값을 가질 수 있다

          => 직사각 행렬에서도 특이(값,벡터)로 분석 가능하지만 주로 정사각 행렬로 정의

          => 1개의 고유벡터를 가질 수도 있음(두개의 고유값이 중복될 경우)

          ex) 3,2 두개의 고유값일 때

          => 3은 가장 큰 고유값으로 변환 후 가장 크게 스케일링 되는 축(가장 큰 변화 일으킨다)

          => 2는 두번째 고유값으로 변환 후 두번째로 크게 스케일링 되는 축


  이 방정식의 의미는 행렬 A가 고유벡터 v를 변환할 때, 고유벡터 v의 방향은 변하지 않고 크기만 λ 배로 변한다는 것이며

  고유벡터는 행렬 변환에 의해 방향이 바뀌지 않는 특별한 벡터를 의미

  구하는 방법

  np.linalg.eig() 함수는 고유값과 고유벡터를 담은 두 개의 배열을 반환




  -=> - 주성분 분석(PCA)

    고유값과 고유벡터는 데이터의 분산을 설명하는 중요한 역할

(고유값은 특정 스칼라 값이고, 고유벡터는 그 고유값에 대응하는 벡터)

(고유값은 각 주성분의 중요도를 나타내며, 데이터의 분산을 설명)

(고유벡터는 각 주성분의 방향을 나타낸다)

    가장 큰 값에 해당하는 고유벡터는 이터 분산의 가장 큰 방향을 나타내며,

    데이터의 주요 패턴을 나타냄

    고유값이 큰 축을 따라 데이터를 변환하여 차원 축소 수행

    고유값이 작은 축은 데이터 분산이 적으므로 무시

    => 정보 손실을 최소화하면서 데이터의 차원을 효과적으로 축소


=========================================================================

  ## 행렬식

    행렬이 나타내는 선형 변환의 스케일링(확장 또는 축소) 비율

      => ex) 행렬식이 2인 행렬은 해당 변환이 2배의 크기로 공간을 확장한다는 것을 의미


      A=   (2 0)
            0  1

      이 행렬은 x축 방향으로 2배로 확장하고, y축 방향으로는 변하지 않는 변환

      행렬 A를 이용해 변환하면, x축 방향으로 2배로 확장되므로

      변환된 사각형의 면적은 다음과 같이 계산

      Transformed area = 2×1= 2

      행렬 A의 행렬식은 2이며, 이는 원래 단위 정사각형의 면적을 2배로 확장한다는 것을 의미

      행렬식은 실제로 해당 행렬이 나타내는 변환과 관련된 면적이나 부피(또는 유사한 고차원 측정값)와 관련

      
      행렬식이 0이 아닌 경우, 행렬은 가역적(역행렬이 존재)

      행렬식이 0이면 행렬은 가역적이지 않으며,

      이는 선형 변환이 공간을 축소시켜 차원을 감소시키는(예: 평면을 선으로 변환) 효과를 가짐을 의미


    - 선형 독립성 판단 : n차 정사각행렬의 행렬식이 0이 아닌 경우, 해당 행렬의 열벡터들이 선형 독립임을 나타낸다

                        이는 행렬을 구성하는 벡터들이 서로 독립적이고, 공간을 완전히 채운다는 것을 의미

    - 특이값 분해 및 주성분 분석 : 행렬식은 데이터의 분산을 설명하는데 도움을 준다

    - 역행렬의 존재 확인 : 역행렬이 존재하는지 여부를 행렬식을 통해 알 수 있다. 행렬식이 0이 아니면 역행렬이 존재

    - 행렬식(determinant)은 행렬을 대표하는 값으로 n x n (n은 2 이상)의 정방행렬 A에 대해 다음과 같이 정의


      detA11이란 A에서 1행과 1열을 제외한 행렬의 행렬식을 의미

      2 x 2 행렬의 요소값이 a,b,c,d라고 할 때 행렬식은 ad−bc

      행렬식 구하는 법

      np.linalg.det(행렬)

=========================================================================


  ## 데이터 분석 및 머신러닝 적용

    - 데이터 표현

      데이터를 벡터 및 행렬 형태로 표현

      ex) 𝑚개의 샘플과 𝑛개의 특성으로 구성된 데이터 셋은 𝑚×𝑛 행렬로 표


    - 회귀 분석

      손실 함수 최소화를 위한 가중치

        y=Xβ+ϵ  형태로 모델링

        X는 특성 행렬,  β  는 가중치 벡터, y는 종속 변수 벡터

        정규 방정식  β=(XTX)−1XTy 로 가중치를 계산

        * np.c_ (배열을 열 방향으로 연결)


    - 주성분 분석

      데이터의 차원을 축소하기 위해 사용

      공분산 행렬의 고유값과 고유벡터를 사용하여 주성분을 찾는다

      고유벡터는 새로운 축을 정의하고, 고유값은 그 축의 중요성을 나타낸다



    - 뉴럴 네트워크

      중치와 입력 벡터의 행렬 곱을 통해 각 층의 출력을 계산

      역전파 알고리즘에서 가중치 행렬의 전치 및 행렬 미분을 사용


    - 클러스터링(군집 분석)

      K-means와 같은 알고리즘에서 데이터 포인트와 클러스터 중심 간의 거리 계산에 벡터 연산을 사용


      클러스터 중심을 갱신할 때, 각 클러스터에 속한 데이터 포인트들의 평균을 계산


    - 신경망

        선형변환

          신경망(Neural Networks) 모델에서 선형 변환의 과정은 입력 데이터가 각 층을 통과하면서 수행되는 선형 연산을 의미

          이를 통해 입력 데이터는 가중치와 편향의 영향을 받아 변환

          선형 변환은 기본적으로 행렬 연산을 사용



        선형 변환의 수식

          z=W⋅X+b 
          z: 선형 변환의 결과 (출력)
          W: 가중치 행렬 (Weights)
          x: 입력 벡터 (Input)
          b: 편향 벡터 (Bias)

과정의 단계별 설명

  입력 데이터 (X):

    입력 데이터는 n개의 특성(feature)으로 구성된 벡터. 예를 들어,  X=[x1,x2,...,xn] 

  가중치 행렬 (W):

    가중치 행렬은 신경망의 각 연결에서 학습된 가중치를 나타낸다

    만약 입력 벡터 x의 차원이 n이고, 다음 층의 출력 차원이 m이라면, 가중치 행렬 W의 크기는 m×n

  편향 벡터 (b):

    편향 벡터는 각 출력 노드에 더해지는 상수로, 모델이 입력 데이터를 보다 잘 맞추도록 도와준다

    편향 벡터 b의 크기는 출력 벡터 z의 크기와 동일. 예를 들어,  b=[b1,b2,...,bm] 

  선형 연산:

    입력 벡터 X와 가중치 행렬 W를 곱하고, 편향 벡터 b를 더한다



    - 신경망 모델

    선형 변환과 비선형 변환을 모두 사용하여 데이터를 처리하고 학습

    기본적으로 다층 구조로 이루어져 있으며,

    각 층은 선형 변환과 비선형 활성화 함수를 결합하여 입력 데이터를 변환


    구성 요소

      * 입력층 (Input Layer):

        모델에 입력되는 데이터. 각 노드는 입력 데이터의 한 특성(feature)을 나타낸다


      * 은닉층 (Hidden Layers):

        입력층과 출력층 사이에 위치한 층들로, 입력 데이터를 선형 변환과 비선형 변환을 통해 처리한다


      * 출력층 (Output Layer):

        모델의 최종 출력. 분류 문제에서는 클래스 확률을 출력하고, 회귀 문제에서는 연속적인 값을 출력


선형 변환 각 층의 노드는 이전 층의 출력에 가중치(weight)를 곱하고, 편향(bias)을 더한 후 결과를 다음 층으로 전달.

이는 다음과 같이 수식으로 표현할 수 있다: z=W⋅x+b 

여기서 W는 가중치 행렬, x는 입력 벡터,b는 편향 벡터.
  
