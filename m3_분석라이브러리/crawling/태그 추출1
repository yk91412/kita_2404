
## 과제 풀이

  class가 content이면서 첫번째 p태그 찾기

    => soup.find('p',class_ = 'content')

    => soup.select_one('p.content')


  다음에 위치한 모든 태그나 문자열 찾기


  => .find_next()



  이전에 위치한 모든 태그나 문자열 찾기


  => .find_previous()


=====================================================================



 - string vs. get_text() vs. text


  (1) string

  - 특징

    요소가 하나의 자식 텍스트 노드를 가지고 있는 경우에만 그 텍스트를 반환

    여러 자식 요소가 있거나 텍스트 노드가 여러 개 있는 경우 None을 반환

  
  (2) get_text() : 메서드

  요소 및 모든 하위 요소의 텍스트를 모두 추출하여 하나의 문자열로 반환


  - 특징
  
  기본적으로 하위 요소 사이에 공백을 추가


  - 옵션

  separator 매개변수를 사용하여 구분자를 지정할 수 있다

    ex) .get_text(separator = '원하는 구분자')


  strip=True 옵션을 사용하여 앞뒤 공백을 제거할 수 있다

    ex) .get_text(strip=True)


  => 함께 사용 가능

    ex) get_text(separator = '원하는 문자열', strip = True)


  (3) text : 속성

  요소 및 모든 하위 요소의 텍스트를 모두 포함하는 문자열을 반환


  - 특징
  
  get_text()와 거의 동일한 결과를 제공

  get_text()와 다르게 추가 매개변수(separator, strip)를 사용할 수 없다


=====================================================================


  - 정규표현식 활용


    re 모듈을 사용하여 특정 패턴을 가진 텍스트를 추출


    ex) 형식

    import re

    pattern = r'~'

    result = re.findall(pattern,문자열)

  => 리스트 형태로 변수에 저장




    ex) 이메일 주소 추출
  
    r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
  
  
    ex) url 추출
  
    r'https?://[^\s<>]+|www\.[^\s<>]+'


    ex) html 태그 내 텍스트 추출

    r'>([^<]+)<'


    ex) 정규 표현식으로 href에서 https인 것 추출하기

    li = soup.find_all(href=re.compile(r"^https://"))
    print(li,'\n')
    for e in li:
        print(e.attrs['href'])


---------------------------------------------

      .: 어떤 한 문자와 일치 (\n 제외)
    
      ^: 문자열의 시작과 일치
    
      $: 문자열의 끝과 일치
    
      *: 0번 이상 반복되는 경우와 일치
    
      +: 1번 이상 반복되는 경우와 일치
    
      ?: 0번 또는 1번 등장하는 경우와 일치
    
      {m,n}: 최소 m번, 최대 n번 반복
    
      []: 문자 집합 중 하나와 일치 (예: [abc]는 a, b, c 중 하나와 일치)
    
      |: OR 조건 (예: a|b는 a 또는 b)
    
      (...): 그룹화
    
      \d: 숫자와 일치
    
      \D: 숫자가 아닌 공백, 문자, 구두점 등 모든 문자와 일치
    
      \s: 스페이스(' '), 탭('\t'), 캐리지 리턴('\r'), 뉴라인('\n'), 폼 피드('\f') 등 공백 문자와 일치
    
      \S: 공백이 아닌 문자, 숫자, 특수 문자 등 모든 것과 일치
    
      \w: 단어 문자(문자, 숫자, 밑줄)와 일치
    
      \W: 단어 문자가 아닌 특수 문자, 공백 문자, 구두점 등과 일치

---------------------------------------------------


=====================================================================


  - urllib + bs


  import urllib.request as rq


  html = requests.get(url)

  html = rq.urlopen(url)

  => 결과는 동일하다




=====================================================================



  - 인코딩 에러 해결

    ** chardet : "Universal Character Encoding Detector"

      => 다양한 인코딩을 감지할 수 있는 파이썬 패키지


    ** chardet.detect(response.content)['encoding']

      => response.content의 인코딩 방식을 자동으로 감지하여 반환

        이 값을 encoding 변수에 저장한 후, response.content를

        encoding 방식으로 디코딩하여 html 변수에 저장하고 출력



=====================================================================


   ** 기사 개수와 크롤링 기사 개수가 다른 이유

    
  (1) 많은 사이트는 JavaScript를 사용하여 콘텐츠를 동적으로 로딩하는 반면

    requests + Beautifulsoup은 기본적으로 JavaScript를 실행하지 않기 때문에

    일부 js로 로딩되는 콘텐츠는 크롤링되지 않는다


  (2) 페이지가 완전히 로드되기 전에 크롤링이 시도될 경우 일부 콘텐츠가 누락될 수 있다


  (3) 웹 페이지가 비동기 요청(Ajax)를 사용하여 데이터를 가져오는 경우

      이 요청을 수동으로 처리하지 않는 한 해당 데이터를 가져오지 못할 수 있다


=====================================================================


  ** CSS 선택자


  원하는 정보만 선별하여 수집하고 싶을 때 css선택자를 활용할 수 있음


- F12 >> 수집하고 싶은 부분 클릭 >> 태그 선택 >> copy Selector

- BeautifulSoup의 select_one, select 활용

  => id나 class

  ex)

  soup.select('body > div > main > section > div > div > ul > li > div > div > strong > a')




=====================================================================


  - enumerate 함수

    start = 1로 인덱스를 1부터 시작하는걸 설정할 수 있다


  - ** format 함수 **

    입력을 받아 url에 활용할 수 있다

    ex) url에 {}을 넣어 url.format(인자 입력)

      url = '~~~{}'

      a = input('입력하세요 : ' )

      a = a.replce(' ', '+')

      -> 띄어쓰기를 +로 하는 이유는 url은 공백을 포함할 수 없고

        검색 엔진이 공백은 +로 표현하므로

      requests.get(url.format(a))



